include './base'
include './f64'
include './cbqnDefs'
include './sse3'
include './avx'
include './avx2'
include './bitops'
include './mask'


def rootty{T & match{typekind{T},'primitive'}} = T
def rootty{T & match{typekind{T},'vector'}} = eltype{T}

# TODO more to some more headerlike file
def ty_dbl{T &  i8==T} = i16
def ty_dbl{T & i16==T} = i32
def ty_dbl{T & i32==T} = i64
def ty_dbl{T & isvec{T}} = [vcount{T}/2](ty_dbl{eltype{T}})
def dcast_i{x} = ext{ty_dbl{type{x}}, x}

# + & -
def arithChk1{F, M, w:T, x:T, r:T & match{F,__add}} = anyneg{M{(w^r) & (x^r)}}
def arithChk1{F, M, w:T, x:T, r:T & match{F,__sub}} = anyneg{M{(w^x) & (w^r)}}
def arithChk1{F, M, w:T, x:T, r:T & match{F,__add} & isvec{T} & width{eltype{T}}<=16} = anyne{__adds{w,x}, r, M}
def arithChk1{F, M, w:T, x:T, r:T & match{F,__sub} & isvec{T} & width{eltype{T}}<=16} = anyne{__subs{w,x}, r, M}

def arithChk2{F, M, w:T, x:T, i & issigned{rootty{T}}} = {
  r:= F{w,x}
  tup{r, arithChk1{F, M, w, x, r}}
}

# ×/∧
def arithChk2{F, M, w:T, x:T, i & match{F,__mul} & match{typekind{T},'primitive'}} = {
  r:= F{dcast_i{w}, dcast_i{x}}
  tup{r, r!=ext{type{r}, trunc{T, r}}}
}

def arithChk2{F, M, w:T, x:T, i & match{F,__mul} & isvec{T} & i16==eltype{T}} = {
  rl:= __mul  {w,x}
  rh:= __mulhi{w,x}
  tup{rl, anyne{rh, rl>>15, M}}
}
def arithChk2{F, M, w:T, x:T, i & match{F,__mul} & isvec{T} & i8==eltype{T}} = {
  def wp = unpackQ{w, T ~~ (broadcast{T,0}>w)}
  def xp = unpackQ{x, T ~~ (broadcast{T,0}>x)}
  def rp = each{__mul, wp, xp}
  def bad = each{{v} => [16]i16 ~~ ((v<<8)>>8 != v), rp}
  if (M{0}) { # masked check
    tup{packQ{rp}, any{M{packQ{bad}}}}
  } else { # unmasked check; can do check in a simpler way
    tup{packQ{rp}, any{tupsel{0,bad}|tupsel{1,bad}}}
  }
}
def arithChk2{F, M, w:T, x:T, i & match{F,__mul} & isvec{T} & i32==eltype{T}} = {
  max:= [8]f32 ~~ broadcast{[8]u32, 0x4efffffe}
  def cf32{x} = emit{[8]f32, '_mm256_cvtepi32_ps', x}
  f32mul:= cf32{w} * cf32{x}
  tup{w*x, any{M{[8]u32 ~~ emit{[8]f32, '_mm256_cmp_ps', abs{f32mul}, max, 29}}}}
  # TODO fallback to the below if the above fails
  # TODO don't do this, but instead shuffle one half, do math, unshuffle that half
  # def wp = unpackQ{w, broadcast{T, 0}}
  # def xp = unpackQ{x, broadcast{T, 0}}
  # def rp = each{__mul32, wp, xp}
  # def T2 = ty_dbl{T}
  # def bad = each{{v}=>{
  #   (((T2~~v) + broadcast{T2,0x80000000}) ^ broadcast{T2, cast{i64,1}<<63}) > broadcast{T2, cast_i{i64, (cast{u64,1}<<63) | 0xFFFFFFFF}}
  # }, rp}
  # tup{packQQ{each{{v} => v&broadcast{T2, 0xFFFFFFFF}, rp}}, any{tupsel{0,bad}|tupsel{1,bad}}} TODO use M
}


# f64
def arithChk3{F, M, w:T, x:T, i} = {
  def r2 = arithChk2{F, M, w, x, i}
  if (rare{tupsel{1,r2}}) return{i}
  tupsel{0,r2}
}

def arithChk3{F, M, w:T, x:T, i & f64==rootty{T}} = F{w,x}

def arithAny{VT, F, W, X, r, len} = {
  def bulk = vcount{VT}
  maskedLoop{bulk, len, {i, M} => {
    rv:= arithChk3{F, M, W{i}, X{i}, i*bulk}
    storeBatch{r, i, rv, M}
  }}
  len
}

def arithAA{VT, F, w, x, r, len} = {
  arithAny{VT, F, {i}=>load{*VT~~w, i}, {i}=>load{*VT~~x, i}, r, len}
}
def arithAS{VT, F, w, x, r, len} = {
  xv:= broadcast{VT, x}
  arithAny{VT, F, {i}=>load{*VT~~w, i}, {i}=>xv, r, len}
}
def arithSA{VT, F, w, x, r, len} = {
  wv:= broadcast{VT, w}
  arithAny{VT, F, {i}=>wv, {i}=>load{*VT~~x, i}, r, len}
}


# cast a guaranteed float to a more specific type; return{0} if not possible
def cast_fB{T, x:(u64) & f64==T} = from_B{f64, x}
def cast_fB{T, x:(u64) & issigned{T} & T<i64} = {
  f:f64 = from_B{f64, x}
  r:T = ftrunc{T, f}
  if (rare{f!=fext{r}}) return{cast{Size,0}}
  r
}

arithAA{F,VT}(w: *u8, x: *u8, r: *u8, len: Size) : Size = { def c{x} = *eltype{VT} ~~ x; arithAA{VT, F, c{w}, c{x}, c{r}, len} }
arithAS{F,VT}(w: *u8, x: u64, r: *u8, len: Size) : Size = { def T=eltype{VT}; arithAS{VT, F, *T ~~ w,       cast_fB{T, x}, *T~~r, len} }
arithSA{F,VT}(w: u64, x: *u8, r: *u8, len: Size) : Size = { def T=eltype{VT}; arithSA{VT, F, cast_fB{T, w}, *T ~~ x,       *T~~r, len} }

'avx2_addAA_i8'  = arithAA{__add,[32]i8 }; 'avx2_addAS_i8'  = arithAS{__add,[32]i8 }; 'avx2_addSA_i8'  = arithSA{__add,[32]i8 }
'avx2_addAA_i16' = arithAA{__add,[16]i16}; 'avx2_addAS_i16' = arithAS{__add,[16]i16}; 'avx2_addSA_i16' = arithSA{__add,[16]i16}
'avx2_addAA_i32' = arithAA{__add,[ 8]i32}; 'avx2_addAS_i32' = arithAS{__add,[ 8]i32}; 'avx2_addSA_i32' = arithSA{__add,[ 8]i32}
'avx2_addAA_f64' = arithAA{__add,[ 4]f64}; 'avx2_addAS_f64' = arithAS{__add,[ 4]f64}; 'avx2_addSA_f64' = arithSA{__add,[ 4]f64}
'avx2_subAA_i8'  = arithAA{__sub,[32]i8 }; 'avx2_subAS_i8'  = arithAS{__sub,[32]i8 }; 'avx2_subSA_i8'  = arithSA{__sub,[32]i8 }
'avx2_subAA_i16' = arithAA{__sub,[16]i16}; 'avx2_subAS_i16' = arithAS{__sub,[16]i16}; 'avx2_subSA_i16' = arithSA{__sub,[16]i16}
'avx2_subAA_i32' = arithAA{__sub,[ 8]i32}; 'avx2_subAS_i32' = arithAS{__sub,[ 8]i32}; 'avx2_subSA_i32' = arithSA{__sub,[ 8]i32}
'avx2_subAA_f64' = arithAA{__sub,[ 4]f64}; 'avx2_subAS_f64' = arithAS{__sub,[ 4]f64}; 'avx2_subSA_f64' = arithSA{__sub,[ 4]f64}
'avx2_mulAA_i8'  = arithAA{__mul,[32]i8 }; 'avx2_mulAS_i8'  = arithAS{__mul,[32]i8 }; 'avx2_mulSA_i8'  = arithSA{__mul,[32]i8 }
'avx2_mulAA_i16' = arithAA{__mul,[16]i16}; 'avx2_mulAS_i16' = arithAS{__mul,[16]i16}; 'avx2_mulSA_i16' = arithSA{__mul,[16]i16}
'avx2_mulAA_i32' = arithAA{__mul,[ 8]i32}; 'avx2_mulAS_i32' = arithAS{__mul,[ 8]i32}; 'avx2_mulSA_i32' = arithSA{__mul,[ 8]i32}
'avx2_mulAA_f64' = arithAA{__mul,[ 4]f64}; 'avx2_mulAS_f64' = arithAS{__mul,[ 4]f64}; 'avx2_mulSA_f64' = arithSA{__mul,[ 4]f64}