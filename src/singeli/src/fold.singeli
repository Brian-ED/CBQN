include './base'
include './mask'
if_inline (hasarch{'BMI2'}) include './bmi2'
include './spaced'

include 'util/tup'

def opsh64{op}{v:([4]f64), perm} = op{v, shuf{[4]u64, v, perm}}
def opsh32{op}{v:([2]f64), perm} = op{v, shuf{[4]u32, v, perm}}
def mix{op, v:([4]f64) if hasarch{'AVX'}} = { def sh=opsh64{op}; sh{sh{v, 4b2301}, 4b1032} }
def mix{op, v:([2]f64) if hasarch{'X86_64'}} = opsh32{op}{v, 4b1032}

def reduce_pairwise{op, plog, x:*T, len, init:T} = {
  # Pairwise combination to shorten dependency chains
  def pairwise{p, i, k} = (if (k==0) { load{p,i} } else {
    def l = k-1
    op{pairwise{p, i       , l},
       pairwise{p, i+(1<<l), l}}
  })
  f:= len >> plog
  r:= init
  @for (i to f) r = op{r, pairwise{x+(i<<plog), 0, plog}}
  @for (x over i from f<<plog to len) r = op{r, x}
  r
}

fn fold_idem{T, op}(x:*T, len:u64) : T = {
  assert{len > 0}
  a := load{x, 0}
  @for (x over _ from 1 to len) a = op{a, x}
  a
}
fn fold_idem{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  r:V = [bulk]f64**0
  assert{len > 0}
  if (len<bulk) {
    # Can't overlap like the long case
    r = load{xv}
    if (bulk>2) {
      assert{(bulk==4) & hasarch{'AVX'}}
      if (len > 1) {
        if (len > 2) r = opsh64{op}{r, 4b2222}
                     r = opsh64{op}{r, 4b1111}
      }
    }
  } else {
    i:= load{*V ~~ (x+len-bulk)}
    r0:= reduce_pairwise{op, 2, xv, (len-1)/bulk, i}
    if (hasarch{'AARCH64'}) return{vfold{op, r0}}
    else r = mix{op, r0}
  }
  extract{r, 0}
}

export{'si_fold_min_f64', fold_idem{f64,min}}
export{'si_fold_max_f64', fold_idem{f64,max}}

fn fold_assoc_0{T, op}(x:*T, len:u64) : T = {
  a:T = 0
  @for (x over len) a = op{a, x}
  a
}
fn fold_assoc_0{T==f64, op if has_simd}(x:*T, len:u64) : T = {
  def bulk = arch_defvw/width{T}
  def V = [bulk]T
  xv:= *V ~~ x
  e:= len / bulk
  i:= load{xv, e} & (V~~maskOf{V, len % bulk})
  r:= reduce_pairwise{op, 2, xv, e, i}
  if (hasarch{'AARCH64'}) vfold{op, r}
  else extract{mix{op, r}, 0}
}
export{'si_sum_f64', fold_assoc_0{f64,+}}


fn xor_words(init:u64, x:*u64, l:usz):u64 = {
  @for (x over l) init ^= x
  init
}
def bit_output{rp:*T} = {
  buf:u64 = 0                    # Buffer for result bits
  def output{i, bit, mod} = {
    buf = buf>>1 | promote{u64, bit}<<63
    if ((i+1)%64==0) { store{rp, 0, mod{buf}}; ++rp }
  }
  def fixbuf{mod} = { buf = mod{buf} }
  def flush_bits{n, mod} = {
    q:=(-n)%64; if (q!=0) store{rp, 0, mod{buf} >> q}
  }
  def flush_bits{n} = flush_bits{n, {b}=>b}
  tup{output, fixbuf, flush_bits}
}
# word and alignment of start of next row
def next_start{i, m} = {
  bn := promote{u64, i+1} * promote{u64, m}
  tup{bn/64, bn%64}
}
fn xor_rows_bit(xp:*u64, rp:*u64, n:usz, m:usz, eq:u1) : void = {
  def p64 = promote{u64, .}
  def fixout = ^{-(p64{eq} &~ p64{m}), .}  # ne to eq conversion
  def {add_bit, _, flush_bits} = bit_output{rp}
  def add_bit{i, bit} = add_bit{i, bit, fixout}
  def xor_loop{len} = {
    o:u64 = 0  # Carry
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, m}
      s := xor_words(o, xp + j, len)
      e := load{xp, jn}
      s ^= e & (if (not same{l,1}) -p64{jn >= j + p64{l}} else j - jn)
      o  = e >> sh
      add_bit{i, popc{s ^ o}}
      j = jn+1
    }
  }
  l := m/64
  if (m < 128) xor_loop{1}
  else if (m%64==0) {
    @for (i to n) add_bit{i, popc{xor_words(0, xp+l*i, l)}}
  }
  else xor_loop{l}
  flush_bits{n, fixout}
}

def or_rows_bit_lt64{xp, rp, n, l, op_and} = {
  nw := cdiv{n*l,64}
  xx := -promote{u64, op_and}
  def loop_pext{T, get, b} = {
    @for (x in xp, r in *T~~rp over nw) r = cast_i{T, pext{get{x}, b}}
  }
  if (l == 2) {
    t:u64 = 64w2b10
    def loop = {
      if (fast_BMI2{}) {
        {op} => loop_pext{u32, {x}=>op{x, x<<1}, t}
      } else {
        def all = 1<<64-1
        ms:5**u64 = reverse{slice{scan{{x,s}=>x^((x<<s)&all), all, reverse{1<<range{6}}}, 1}}
        {op} => {
          def T = u32
          @for (x in xp, r in *T~~rp over nw) {
            a := op{x, x>>1}
            each{{m, sh} => { a &= m; a |= a>>sh }, ms, 1<<range{5}}
            r = cast_i{T, a}
          }
        }
      }
    }
    if (op_and) loop{&} else loop{|}
  } else if (l == 4) {
    m:u64 = 64w2b0001; t := m<<3
    def loop = {
      if (fast_BMI2{}) {
        loop_pext{u16, ., t}
      } else {
        {get} => {
          def T = u16
          @for (x in xp, r in *T~~rp over nw) {
            a := get{x} & t
            a = (a * 4r2b001) & (64w0xf000)
            a = (a * fold{+, 1<<(12*iota{4})}) >> (3*16)
            r = cast_i{T, a}
          }
        }
      }
    }
    if (op_and) loop{{x} => x & ((x&~t) + m)}
    else        loop{{x} => x | ((x| t) - m)}
  } else {
    {e0, d} := unaligned_spaced_mask_mod{l}
    e := e0 << (l-1)
    r:u64 = 0
    rh := *u32~~rp
    ri:ux = 0
    if (fast_BMI2{}) {
      @for (xo in xp over i to nw) {
        m := e<<1 | 1
        t := e | 1<<63
        x := xo^xx
        r |= pext{x | ((x|t) - m), t} << ri
        ri += popc{e}
        e = e>>d | e<<(l-d)
        if (ri >= 32) {
          store{rh, 0, cast_i{u32,r^xx}}; ++rh;
          r >>= 32; ri -= 32
        }
      }
    } else {
      dm:= cast_i{usz, popc{e}}
      c:u64 = 0
      def loop{...par} = {
        @for (xo in xp over nw) {
          m := e<<1 | 1
          t := e | (1<<63)
          x := xo^xx
          s := x | ((x|t) - m)          # Fold results
          cs:= c; c = (s&~e) >> 63
          nb:= dm + promote{usz, e>=e0} # = popc{e}
          def extract = match {
            {{...qs, q}, {...bs, b}} => (extract{qs, bs} & b) * q
            {       {q},         {}} => (s & e) * (q << clz{e})
          }
          rb:= extract{...par} >> (64 - nb)
          r |= (rb|cs) << ri
          ri += promote{ux, nb}
          if (ri >= 32) {
            store{rh, 0, cast_i{u32,r^xx}}; ++rh;
            r >>= 32; ri -= 32
          }
          e = e>>d | e<<(l-d)
        }
      }
      if (l == 3) {
        mult0:u64 = base{1<< 2, 3**1};  top3:u64 = base{1<<9,  8**(1<<3-1)}>>2
        mult1:u64 = base{1<< 6, 3**1};  top9:u64 = base{1<<27, 3**(1<<9-1)}<<1
        mult2:u64 = base{1<<18, 3**1}
        loop{tup{mult0,mult1,mult2}, tup{top3, top9}}
      } else if (l < 8) {
        assert{l > 4}
        ld:= l-1; lld:= l*ld
        {mult0, _} := unaligned_spaced_mask_mod{ld}
        mult0 &= u64~~1<<lld - 1
        {mult1, _} := unaligned_spaced_mask_mod{lld}
        ll:= l*l
        {tk, tkd} := unaligned_spaced_mask_mod{ll}; tk <<= tkd
        topk := tk - tk>>l; topk|= topk<<ll | topk>>ll
        loop{tup{mult0,mult1}, tup{topk}}
      } else {
        {mult, _} := unaligned_spaced_mask_mod{l-1}
        loop{tup{mult}, tup{}}
      }
    }
    if (ri > 0) store{rh, 0, cast_i{u32,r^xx}}
  }
}

fn or_rows_bit(xp:*u64, rp:*u64, n:usz, l:usz, op_and:u1) : void = {
  def {add_bit, set_out, flush_bits} = bit_output{rp}
  if (l < 64) {
    or_rows_bit_lt64{xp, rp, n, l, op_and}
    return{}
  } else if (l < 128) {
    c:u64 = (promote{u64, l}-1) &- op_and # a row gives 1 if its sum is >c
    o:u64 = 0
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      s := o + popc{load{xp,j}}
      e := load{xp,jn}
      s += popc{e & (j - jn)}  # mask is 0 if j==jn, or -1
      o  = popc{e >> sh}
      add_bit{i, s > c+o, {rw}=>rw}
      j = jn+1
    }
  } else {
    rx := -promote{u64, op_and}; id := ~rx
    def fixout = ^{rx, .}
    o:u64 = 0  # Saved bits
    j:u64 = 0; @for (i to n) {
      def {jn, sh} = next_start{i, l}
      e := load{xp,jn} ^ rx
      l := ~(u64~~0) << sh
      rb:u64 = 1
      if ((o | (e &~ l)) == 0) { # Search for shortcut
        @for (i from j to jn-1) if (load{xp,i} != id) goto{'found'}
        rb = 0; setlabel{'found'}
      }
      o = e & l
      add_bit{i, rb, fixout}
      j = jn+1
    }
    set_out{fixout}
  }
  flush_bits{n}
}
export{'si_xor_rows_bit', xor_rows_bit}
export{'si_or_rows_bit', or_rows_bit}
