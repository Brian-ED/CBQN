include './base'
include './mask'
include './spaced'

def ind_types = tup{i8, i16, i32}
def dat_types = tup{...ind_types, u64}

# Indices and Replicate using plus- or max-scan
def scan_core{upd, set, scan, rp:*T, wp:W, s:(usz)} = {
  def getw{j} = if (isptr{W}) cast_i{usz,load{wp,j}} else wp
  b:usz = 1<<10
  k:usz = 0; j:usz = 0; ij:=getw{j}
  while (1) {
    e := tern{b<s-k, k+b, s}
    @for (rp over i from k to e) rp = 0
    if (set) store{rp, k, cast_i{T,j}}
    while (ij<e) { ++j; upd{rp, j, ij}; ij+=getw{j} }
    scan{rp+k, e-k}
    if (e==s) return{}
    k = e
  }
}
def indrep_by_sum{T, rp:*T, wp, s:(usz), js, inc} = {
  def scan{ptr, len} = @for (ptr over len) js=ptr+=js
  def scan{ptr:*E, len if width{T}<=32} = {
    def scanfn = merge{'si_scan_pluswrap_u',fmtnat{width{T}}}
    p := *ty_u{E}~~ptr
    emit{void, scanfn, p, p, len, js}; js=load{ptr,len-1}
  }
  def upd{rp, j, ij} = store{rp, ij, load{rp,ij}+inc{j}}
  scan_core{upd, 0, scan, rp, wp, s}
}

fn ind_by_scan_i32{W}(xv:*void, rp:*i32, s:usz) : void = {
  xp := *W~~xv
  if (hasarch{'X86_64'} and not hasarch{'SSE4.1'}) { # no min instruction
    js:i32 = 0
    indrep_by_sum{i32, rp, xp, s, js, {j}=>1}
  } else {
    scan_core{
      {rp,j,ij} => store{rp,ij,cast_i{i32,j}}, 1,
      {ptr,len} => emit{void, 'si_scan_max_i32', ptr,ptr,len},
      rp, xp, s
    }
  }
}

def rep_by_scan{T, wp, xv:(*void), rv:(*void), s} = {
  xp := *T~~xv; js := *xp; px := js
  def inc{j} = {sx:=px; px=load{xp,j}; px-sx}
  indrep_by_sum{T, *T~~rv, wp, s, js, inc}
}
fn rep_by_scan{W, T}(wp:*void, xv:*void, rv:*void, s:usz) : void = {
  rep_by_scan{T, *W~~wp, xv, rv, s}
}

exportT{'si_indices_scan_i32', each{ind_by_scan_i32, ind_types}}
exportT{'si_replicate_scan', flat_table{rep_by_scan, ind_types, dat_types}}


# Constant replicate
if_inline (not (hasarch{'AVX2'} or hasarch{'AARCH64'})) {

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  rep_by_scan{T, cast_i{usz,wv}, x, r, cast_i{usz, wv*n}}
}

} else {

def incl{a,b} = slice{iota{b+1},a}

# 1+˝∨`⌾⌽0=div|⌜range
def makefact{divisor, range} = {
  def t = table{{a,b}=>0==b%a, divisor, range}
  fold{+, 1, reverse{scan{|, reverse{t}}}}
}
def basic_rep = incl{2, 7}
def fact_size = 128
def fact_inds = slice{iota{fact_size},8}
def fact_tab = makefact{basic_rep, fact_inds}
factors:*u8 = fact_tab



def sdtype = [arch_defvw/8]i8 # shuf data type
def get_shufs{step, wv, len} = {
  def i = iota{len*step}
  split{step, (i - i%wv)/wv}
}
def get_shuf_data{wv, len} = get_shufs{vcount{sdtype}, wv, len} # [len] byte-selector vectors for wv/sdtype (expanded to wider types by read_shuf_vecs)
def get_shuf_data{wv} = get_shuf_data{wv, wv}

# all shuffle vectors for 𝕨≤7
def special_2 = ~hasarch{'AARCH64'} # handle 2 specially on x86-64
def rcsh_vals = slice{basic_rep, special_2}
rcsh_offs:*u8 = shiftright{0, scan{+,rcsh_vals}}
rcsh_data:*i8 = join{join{each{get_shuf_data, rcsh_vals}}}

# first 4 shuffle vectors for 11≤𝕨≤61; only uses the low half of the input
def rcsh4_dom = replicate{bind{>=,64}, replicate{fact_tab==1, fact_inds}}
rcsh4_dat:*i8 = join{join{each{get_shuf_data{., 4}, rcsh4_dom}}}
rcsh4_lkup:*i8 = shiftright{0, scan{+, fold{|, table{==, rcsh4_dom, iota{64}}}}}

def read_shuf_vecs{l, ellw:(u64), shp:*V} = { # tuple of byte selectors in 1<<ellw
  def double{x:X if hasarch{'AVX2'}} = {
    s:=shuf{[4]u64, x, 4b3120}; s+=s
    r:=each{bind{~~,[32]i8},mzip128{s, s + X**1}}
    r
  }
  def double{x:X if hasarch{'AARCH64'}} = {
    s:= x+x
    zip{s, s + X**1}
  }
  def doubles{n,tup} = slice{join{each{double,tup}}, 0, n}
  
  def sh = each{{v}=>{r:=v}, l**V**0}
  def tlen{e} = cdiv{l, e}  # Length for e bytes, rounded up
  def set{i} = { select{sh,i} = each{load{shp,.}, i} }
  def ext{e} = {
    def m = tlen{2*e}; def n = tlen{e}  # m<n
    if (ellw <= lb{e}) set{slice{iota{n},m}}
    else slice{sh,0,n} = doubles{n,slice{sh,0,m}}
  }
  set{iota{tlen{8}}}; ext{4}; ext{2}; ext{1}
  sh
}

def rep_const_shuffle{wv, onreps, xv:*V=[step]T, rv:*V, n:(u64)} = { # onreps{inputVector, {nextOutputVector} => ...}
  nv := n / step
  j:u64 = 0
  def write{v} = { store{rv, j, v}; ++j }
  @for (xv over nv) onreps{xv, write}
  if (nv*step < n) {
    nr := n * wv
    e := nr / step
    s := V**0
    def end = makelabel{}
    onreps{load{xv,nv}, {v} => {
      s = v
      if (j == e) goto{end}
      write{s}
    }}
    setlabel{end}
    q := nr & (step-1)
    if (q!=0) homMaskStoreF{rv+e, maskOf{V, q}, s}
  }
}

if_inline (hasarch{'AVX2'}) {
  def rep_iter_from_sh{sh}{x, gen} = {
    def l = length{sh}
    def h = l>>1
    def fs{v, s} = gen{sel{[16]i8, v, s}}
    a := shuf{[4]u64, x, 4b1010}; each{fs{a,.}, slice{sh,0,h}}
    if (l%2) fs{x, select{sh, h}}
    b := shuf{[4]u64, x, 4b3232}; each{fs{b,.}, slice{sh,-h}}
  }
  
  def get_rep_iter{V, wv==2}{x, gen} = {
    def s = shuf{[4]u64, x, 4b3120}
    each{{q}=>gen{V~~q}, mzip128{s, s}}
  }
  def get_rep_iter{V==[4]u64, wv} = {
    def step = 4
    def sh = each{base{4,.}, get_shufs{step, wv, wv}}
    {x, gen} => each{{s}=>gen{shuf{V, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv, xv:*V, rv:*V, n:(u64)} = rep_const_shuffle{wv, get_rep_iter{V, wv}, xv, rv, n}
  
} else if_inline (hasarch{'AARCH64'}) {
  
  def rep_iter_from_sh{sh}{x, gen} = {
    each{{s} => gen{sel{[16]u8, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv==2, xv0:*V=[_]T, rv0:*V, n:(u64)} = {
    def E = ty_u{T}
    rv:= *E~~rv0
    @for (x in *E~~xv0 over i to n) { # autovectorized well enough, probably
      store{rv, i*2, x}
      store{rv, i*2+1, x}
    }
  }
}

fn rep_const_shuffle_partial4(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  def h = 4
  def V = sdtype
  def sh = read_shuf_vecs{h, ellw, *V~~rcsh4_dat + h*load{rcsh4_lkup,wv}}
  def [step]_ = V          # Bytes written
  def wvb = wv << ellw
  def hs = (h*step) / wvb  # Actual step size in argument elements
  def shufbase{i if hasarch{'AVX2'}} = shuf{[4]u64, load{*V~~(x+i)}, 4b1010}
  def shufbase{i if hasarch{'AARCH64'}} = load{*V~~(x+i)}
  def shufrun{a, s} = sel{[16]i8, a, s} # happens to be the same across AVX2 & NEON
  
  i:u64 = 0
  re := r + n*wvb - h*step
  while (r <= re) {
    a := shufbase{i}
    @unroll (j to h) store{*V~~r, j, shufrun{a, select{sh,j}}}
    i += hs << ellw
    r += hs*wvb
  }
  re+= (h-1)*step
  a:= shufbase{i}
  s:= V**0
  
  def end = makelabel{}
  @unroll (j to h) {
    s = shufrun{a, select{sh,j}}
    if (r > re) goto{end}
    store{*V~~r, 0, s}
    r+= step
  }
  setlabel{end}
  
  q := (re+step) - r
  if (q!=0) homMaskStoreF{*V~~r, maskOf{V, q}, s}
}



fn rcsh_sub{wv, V}(ellw:u64, x:*i8, r:*i8, n:u64, sh:*V) : void = {
  def st = read_shuf_vecs{wv, ellw, sh}
  rep_const_shuffle{wv, rep_iter_from_sh{st}, *V~~x, *V~~r, n}
}

fn rep_const_shuffle_any(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  if (wv > select{rcsh_vals,-1}) {
    return{rep_const_shuffle_partial4(wv, ellw, x, r, n)}
  }
  n <<= ellw
  ri := wv - select{rcsh_vals,0}
  sh := *sdtype~~rcsh_data + load{rcsh_offs,ri}
  def try{k} = { if (wv==k) rcsh_sub{k, sdtype}(ellw, x, r, n, sh) }
  each{try, rcsh_vals}
}

def rep_const_broadcast{T, kv, loop, wv:(u64), x:*T, r:*T, n:(u64)} = {
  assert{kv > 0}
  def V = [arch_defvw/width{T}]T
  @for (x over n) {
    v := V**x
    @loop (j to kv) store{*V~~r, j, v}
    r += wv
    store{*V~~r, -1, v}
  }
}
fn rep_const_broadcast{T, kv    }(wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, unroll, wv, x, r, n}
fn rep_const_broadcast{T}(kv:u64, wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, for   , wv, x, r, n}

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  assert{wv>=2}
  if (wv>=8 and wv<=fact_size) {
    fa := promote{u64, load{factors,wv-8}}
    if (fa > 1) {
      fi := wv / fa
      def t = *void~~(*T~~r + (promote{u64,wv}-fi)*n)
      rep_const{T}(fi,x,t,n)
      rep_const{T}(fa,t,r,fi*n)
      return{}
    }
  }
  def wT = width{T}
  def vn = arch_defvw/wT
  def V = [vn]T
  def max_shuffle = 2*vn
  if (wv <= max_shuffle) {
    def specialize{k} = {
      if (wv==k) return{rep_const_shuffle{k, *V~~x, *V~~r, n}}
    }
    if (special_2) specialize{2}
    rep_const_shuffle_any(wv, lb{wT/8}, *i8~~x, *i8~~r, n)
  } else {
    kv := wv / vn
    @unroll (k from (max_shuffle/vn) to 4) {
      if (kv == k) return{rep_const_broadcast{T, k}(wv, *T~~x, *T~~r, n)}
    }
    rep_const_broadcast{T}(kv, wv, *T~~x, *T~~r, n)
  }
}

}

exportT{'si_constrep', each{rep_const, dat_types}}



# Constant replicate on boolean
fn rep_const_bool{}(wv:usz, x:*u64, r:*u64, rlen:usz) : u1 = {
  if (wv > 64) return{0}
  nw := cdiv{rlen, 64}
  if (wv&1 == 0) {
    p := ctz{wv | 8} # Power of two for second replicate
    wf := wv>>p
    if (wf == 1) {
      rep_const_bool_div8{wv, x, r, nw}
    } else {
      tlen := rlen>>p
      wq := usz~~1<<p
      if ((not hasarch{'SSSE3'} and (p == 1 or (p == 2 and wv>=52))) or (p == 1 and wv>=24)) {
        # Expanding odd second is faster
        tlen = rlen / wf
        t:=wf; wf=wq; wq=t
      }
      t := r + cdiv{rlen, 64} - cdiv{tlen, 64}
      rep_const_bool{}(wf, x, t, tlen)
      rep_const_bool{}(wq, t, r, rlen)
    }
  } else {
    rep_const_bool_odd{wv, x, r, nw}
  }
  1
}

def rep_const_bool_div8{wv, x, r, nw} = { # wv in 2,4,8
  def run{k} = {
    # 2 -> 64w0x33, 12 -> 64w0x000f, etc.
    def getm{sh} = base{2, iota{64}&sh == 0}
    def osh{v, s} = v | v<<s
    def expand = match (k) {
      {2} => fold{
        {v, sh} => osh{v, sh} & getm{sh},
        ., 1 << reverse{iota{5}}
      }
      {4} => fold{
        {v, sh} => osh{osh{v, sh}, 2*sh} & getm{sh},
        ., tup{12, 3}
      }
      {8} => {
        def mult = base{1<<7, 8**1}
        {x} => (x | ((x&~1) * mult)) & 64w0x01
      }
    }
    @for (xt in *ty_u{64/k}~~x, r over nw) {
      def v = expand{promote{u64, xt}}
      r = v<<k - v
    }
  }
  def cases{k} = if (wv==k) run{k} else if (k<8) cases{2*k}
  cases{2}
}

def get_boolvec_writer{V, r, nw} = {
  def vwords = width{V}/64
  rv := *V~~r
  re := rv + nw / vwords
  rs := rv + cdiv{nw, vwords}
  # Avoid reading/processing an extra word past the end of input
  def done = makelabel{}
  def check_done{} = if (rv == rs) goto{done}
  # If the last result is partial, jump to flush to write it
  last_res:V = V**0
  def l_flush = makelabel{}
  def output{v:(V)} = {
    last_res = v
    if (rv == re) goto{l_flush}
    store{rv, 0, v}; ++rv
  }
  def flush{} = {
    setlabel{l_flush}
    q := nw & (vwords-1)
    if (q != 0) homMaskStoreF{rv, V~~maskOf{re_el{u64,V}, q}, last_res}
    setlabel{done}
  }
  tup{output, check_done, flush}
}
def get_boolvec_writer{T=(u64), r:*T, nw} = {
  def done = makelabel{}
  j:usz = 0
  def output{rw} = {
    store{r, j, rw}
    ++j; if (j==nw) goto{done}
  }
  def flush{} = setlabel{done}
  tup{output, {}=>{}, flush}
}

def rep_const_bool_div8{wv, x, r, nw if hasarch{'SSSE3'}} = {
  def avx2 = hasarch{'AVX2'}
  def vl = if (avx2) 32 else 16
  def V = [vl]u8
  def iV = iota{vl}
  def mkV = make{V, .}
  def selH = sel{[16]u8, ., .}
  def makeTab{t} = selH{mkV{if (avx2) merge{t,t} else t}, .}
  def id{xv} = xv
  def {output, done, flush} = get_boolvec_writer{V, r, nw}

  def run24{x, proc_xv, exh} = {
    i:usz = 0; while (1) {
      xv := proc_xv{load{*V~~(x+i)}}; ++i
      # Store 1 or 2 result vectors
      def getr = zip128{exh{xv}, exh{V~~(re_el{u16,V}~~xv>>4)}, .}
      output{V~~getr{0}}
      output{V~~getr{1}}
    }
  }
  if (wv == 2) {
    def init = if (avx2) shuf{[4]u64, ., 4b3120} else id
    # Expander for half byte
    def tabr = makeTab{tr_iota{2*iota{4}} * 2b11}
    m4 := V**0xf
    run24{*V~~x, init, {x} => tabr{x & m4}}
  } else if (wv == 4) {
    # Unzip 32-bit elements (result lanes) across AVX2 lanes
    def pre = if (avx2) sel{[8]u32, ., make{[8]u32,tr_iota{1,2,0}}} else id
    def init{xv} = { u:=pre{xv}; zip128{u,u,0} }
    # Expander for two bits in either bottom or next-to-bottom position
    def tabr = makeTab{tr_iota{0,4,0,4} * 2b1111}
    m2 := mkV{2b11 << (2*(iV%2))}
    def exh{x} = re_el{u16, V}~~tabr{x & m2}
    run24{*(if (avx2) [2]u64 else u64)~~x, init, exh}
  } else { # wv == 8
    i:usz = 0; while (1) {
      xh := load{*[16]u8~~(*ty_u{vl}~~x + i)}; ++i
      xv := if (avx2) pair{xh, xh} else xh
      xe := selH{xv, mkV{iV // 8}}
      output{(xe & mkV{1 << (iV % 8)}) > V**0}
    }
  }
  flush{}
}

def sel_imm{V=[4]T, x, {...inds} if hasarch{'SSE2'} and (T==u32 or T==u64)} = {
  assert{length{inds} == 4}
  shuf{V, x, base{4, inds}}
}
def sel_imm{V=([16]u8), x:X, {...inds}} = {
  def I = re_el{u8,X}; def [n]_ = I
  def l = length{inds}
  assert{l==16 or l==n}
  sel{V, x, make{I, cycle{n, inds}}}
}

def advance_spaced_mask{k, m, sh} = m<<(k-sh) | m>>sh

# Data for the permutation that sends bit i to k*i % width{T}
def modperm_dat{T, k} = {
  def w = width{T}
  def i = iota{lb{w}}
  def bits = ~(1 & (k*iota{w} >> merge{0, replicate{1<<i, i}}))
  match (T) {
    {[_]E} => make{T, each{base{2,.}, split{width{E}, bits}}}
    {_}    => T~~base{2, bits}
  }
}
# Permutation step evaluators
# shift takes a top-half mask; others take both-halves
def modperm_shift_step{x, l, m} = {
  def d = (x ^ x<<l) & m
  x ^ (d | d>>l)
}
def modperm_rot_step{x:T, l=(width{T}/2), m} = {
  (x &~ m) | ((x<<l | x>>l) & m)
}
def modperm_shuf_step{x:V=[_]T, l, m if l%8==0} = {
  (x &~ m) | (swap_elts{x, l/8} & m)
}
# Reverse each pair of elements
def swap_elts{x:V=[_]_, el_bytes} = {
  def selx{n, wd} = {
    def l = el_bytes/wd
    def i = iota{n}
    def rev = i + (l - 2*(i&l))
    sel_imm{[n]ty_u{wd*8}, x, rev}
  }
  if (el_bytes >= 4) {
    selx{4, max{4, el_bytes/2}}
  } else if (hasarch{'SSSE3'}) {
    selx{16, 1}
  } else {
    def sh = el_bytes*8
    xe := re_el{ty_u{2*sh}, V} ~~ x
    V~~(xe<<sh | xe>>sh)
  }
}
# Extract swap functions from modperm_dat
def extract_modperm_mask{full:W, lane, l} = {
  def f = l == 128                           # Use full width
  def w = if (l<32) 8 else if (f) 64 else 32 # Shuffle element width
  def n = (if (f) 256 else 128) / w          # and number of elements
  def o = l / w                              # Extract [o,2*o)
  def i = o + iota{n}%o
  sel_imm{[n]ty_u{w}, if (f) full else lane, i}
}
def proc_mod_dat{swap_data:W} = {
  def ww = width{W}; def avx2 = ww==256
  def on_len_range{get_swap, lo, hi} = {
    def lens = reverse{1 << slice{iota{lb{hi}}, lb{lo}}}
    fold{{x,s}=>s{x}, ., each{get_swap, lens}}
  }
  swap_lane := if (avx2) shuf{W, swap_data, 4b1010} else swap_data
  # Transform width-w units with shifts only
  def get_shiftperm{data, w} = {
    dat := data
    bot:W = W**base{2, cycle{64, replicate{w, tup{1,0}}}}
    def gsw{l} = {
      bot ^= bot << l  # Low l bits out of every 2*l
      sm := dat &~ bot
      dat &= bot; dat |= dat<<l
      if (l<32) modperm_shift_step{., l, sm}
      else      modperm_rot_step  {., l, sm|sm>>l}
    }
    on_len_range{gsw, 2, w}
  }
  # Within-byte transformation
  def get_byteperm{} = {
    def V = re_el{u8, W}
    def sw_bytes = sel{[16]u8, swap_lane, V**0}
    m4 := W~~V**0xf
    t0 := fold{{v,a}=>modperm_shift_step{v,...a}, W~~make{V,iota{vcount{V}}%16}, tup{
      tup{4, sw_bytes &~ m4},
      tup{2, ({v} => v | v<<4){sw_bytes & W~~V**0xc}}
    }}
    t4 := (t0&m4)<<4 | (t0&~m4)>>4
    def selI{v,i} = sel{[16]u8, v, V~~i}
    {xv} => selI{t0, xv & m4} | selI{t4, (xv>>4) & m4}
  }
  def {partwidth, partperm} = {
    def sh{w, d} = tup{w, get_shiftperm{d, w}}
    if      (ww==64)               sh{64, swap_data}
    else if (not hasarch{'SSSE3'}) sh{32, shuf{[4]u32, swap_data, 4b0000}}
    else                           tup{8, get_byteperm{}}
  }
  # Fill in higher steps
  def get_mod_permuter{} = {
    def get_swap{l} = {
      def mask = extract_modperm_mask{swap_data, swap_lane, l}
      modperm_shuf_step{., l, mask}
    }
    def swap_x = on_len_range{get_swap, partwidth, ww}
    {x} => partperm{swap_x{x}}
  }
  tup{partperm, get_mod_permuter}
}

def rep_const_bool_odd{k, x, r, nw} = {
  def avx2 = hasarch{'AVX2'}
  def W = if (hasarch{'SSE2'}) [if (avx2) 4 else 2]u64 else u64

  def {output, check_done, flush} = get_boolvec_writer{W, r, nw}
  xp := *W~~x
  def getter{perm}{} = { check_done{}; xv := load{xp}; ++xp; perm{xv} }

  # Modular permutation: small-k cases may use a limited permutation
  # on bytes or 32-bit ints; general case uses the whole thing
  swtab:*W = each{modperm_dat{W, .}, 1+2*iota{32}}
  swap_data := load{swtab, k>>1}
  def {partperm, get_full_permute} = proc_mod_dat{swap_data}

  def sp_max = if (hasarch{'SSSE3'} and not avx2) 8 else 4
  if (k < sp_max) {
    rep_const_bool_small_odd{W, sp_max, k, getter{partperm}, output}
  } else {
    def get_swap_x = getter{get_full_permute{}}
    rep_const_bool_odd_mask4{W, k, get_swap_x, output, cdiv{nw, vcount{W}}}
  }
  flush{}
}

# General-case loop for odd replication factors
def rep_const_bool_odd_mask4{
  M,             # read/write type
  k,             # replication factor
  get_modperm_x, # permuted input
  output, n,     # output, number of writes
} = {
  def ifvec{g} = match (M) { {[_](u64)} => g; {_} => ({v}=>v) }
  def scal = ifvec{{v} => M**v}

  # Fundamental operation: shifts act as order-k cyclic group on masks
  def advance{m, sh} = advance_spaced_mask{k, m, sh}
  # Double a cumulative mask, shift combination
  # If s advances l iterations, mc combines iterations iota{l}
  def double_gen{comb}{{mc, s}} = {
    def mn = comb{mc, advance{mc,s}}
    def ss = s+s
    tup{mn, ss - (k &- (ss>k))}
  }
  # Starting word mask and single-word shift
  {mask, mask_sh} := unaligned_spaced_mask_mod{k}
  # Mask and shift for one iteration
  def fillmask{T} = match (T) {
    {(u64)} => tup{mask, mask_sh}
    {[2]E}  => double_gen{make{T,...}}{fillmask{E}}
    {[4]E}  => double_gen{pair}{fillmask{[2]E}}
  }
  {sm0, s1} := fillmask{M}
  # Combined mask for 4 iterations, and shift to advance 4
  def double = double_gen{|}
  {mc4, s4} := double{double{tup{sm0, s1}}}
  # Submasks pick one mask out of a combination of 4
  def or_adv{m, s} = { m |= advance{m,s} }
  @for (min{k/4 - 1, n/4}) or_adv{sm0,s4}
  submasks := scan{advance, tup{sm0, ...3**s1}}
  mask_tail := advance{sm0, s4} &~ sm0

  # Carry: shifting and word-crossing is done on the initial permuted x
  # No need to carry across input words since they align with output words
  # First bit of each word in xo below is wrong, but it doesn't matter!
  mr := scal{u64~~1<<k - 1}  # Mask out carry bit before output
  def sub_carry{a, c} = match (M) {
    {[l](u64)} => {
      ca := if (hasarch{'SSE4.2'}) { def S = [l]i64; S~~c > S**0 }
            else { def S = [2*l]i32; cm := S~~c; cm != shuf{S, cm, 4b2301} }
      a + M~~ca
    }
    {_} => a - promote{u64, c != 0}
  }

  while (1) {
    x:M = get_modperm_x{}
    def vrot1 = ifvec{{x} => if (w128{M}) shuf{[4]u32, x, 4b1032}
                             else shuf{M, x, 4b2103}}
    xo := x<<k | vrot1{x>>(64-k)}
    # Write result word given starting bits
    def step{b, c} = output{sub_carry{c - b, c & mr}}
    def step{b, c, m} = step{b&m, c&m}
    # Fast unrolled iterations
    mask := mc4
    @for (k/4) {
      each{step{x & mask, xo & mask, .}, submasks}
      mask = advance{mask, s4}
    }
    # Single-step for tail
    mask = mask_tail
    @for (k%4) {
      step{x, xo, mask}
      mask = advance{mask, s1}
    }
  }
}

def rep_const_bool_small_odd{(u64), 4, wv, get_swap_x, output} = {
  def k = 3
  def step{x}{o, n_over} = {
    b := x & base{2, cycle{64, n_over == iota{k}}}
    def os = (64-k) + 1 + iota{n_over}
    output{fold{|, (b<<k) - b, each{>>{o,.}, os}}}
    b
  }
  while (1) fold{step{get_swap_x{}}, 0, (-iota{k}*64)%k}
}

# For small odd numbers:
# - permute each byte sending bit i to position k*i % 8
# - replicate each byte by k, making position k*i contain bit i
# - mask out those bits and spread over [ k*i, k*(i+1) )
# - ...except where it crosses words; handle this overhang separately
# If 1-byte shuffle isn't available, use 4-byte units instead
def rep_const_bool_small_odd{W=[wl](u64), max_wv, wv, get_perm_x, output} = {
  def ov_bytes{o} = { def V = re_el{u8, W}; v := V~~o; v += v > V**0; W~~v }
  if (max_wv <= 4 or wv < 4) {
    def ww = width{W}
    def ew = if (hasarch{'SSSE3'}) 8 else 32  # width of shuffle-able elements
    def ne = ww/ew; def se = 64/ew
    def lanes = ww > 128
    def dup{v} = if (lanes) merge{v,v} else v
    # 3: dedicated loop
    while (1) {
      # 01234567 to 05316427 on each byte
      xv := get_perm_x{}
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{3},1} // 3  # bits that overhang within a word
      def ib = ix // ew                  # byte index
      def io = ew*ib + 3*ix%ew           # where they are in xv
      def wi = split{wl, dup{tup{255, ...ib, 255, ...se+ib}}}
      xo := ov_bytes{(xv & W**fold{|, 1<<io}) >> (ew-3)}
      # Permute and mask bytes
      def step{jj, oi, ind, mask} = {
        def getv = if (not lanes or jj==1) ({x}=>x)
                   else shuf{[4]u64, ., 4b1010 + 4b2222*(jj>1)}
        def selx{x, i} = sel_imm{[128/ew]ty_u{ew}, getv{x}, i}
        b := selx{xv, ind} & make{W, mask}
        r := (b<<3) - b
        def selx_nz{x, i} = { def nz = i!=255; selx{x, i * nz} & W~~make{[4]i32, -nz} }
        o := (if (ew==8) selx else selx_nz){xo, flat_table{max, oi, 255*(0<iota{se})}}
        output{r|o}
      }
      each{step,
        iota{3}, wi,
        split{ne, replicate{3, iota{ne}}},
        split{wl, each{base{2,.}, split{64, cycle{3*ww, 0==iota{3}}}}}
      }
    }
  } else {
    assert{w128{W}}
    def V = re_el{u8, W}; def [vl]_ = V
    def mkV = make{V, .}; def selV = sel{V, ., .}
    def iV = iota{vl}
    # 5, 7: precompute constants, then shared loop
    {xom, xse, ind0, mask0, ind_up, ind_inc, mask_sh} := undef{tup{
     W,   V,   V,    W,     V,      V,       usz   }}
    def set_consts{k} = {
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{k},1} // k  # bits that overhang within a word
      def ib = ix // 8                   # byte index
      def io = 8*ib + k*ix%8             # where they are in xv
      def wi = tup{255, ...ib, 255, ...8+ib}
      xom = W**fold{|, 1<<io}
      xse = mkV{join{flip{split{2, shiftright{wi, vl**255}}}}}
      # Permutation to expand by k bytes, and every-k-bits mask
      ind0  = mkV{iV // k}
      mask0 = W~~mkV{((1<<k|1) << ((-8)*iV % k)) % 256}
      def iu = iV + vl%k; def ia = iu>=vl
      ind_up  = mkV{iu - k*ia}
      ind_inc = mkV{vl//k + ia}
      mask_sh = width{V} % k
    }
    if (wv == 5) set_consts{5} else set_consts{7}
    xv:=W**0; xo:=xv; mask:=xv; ind:=V**0  # state
    q:usz = 1
    while (1) {
      --q; if (q == 0) { q = wv
        # Load and permute bytes
        xv = get_perm_x{}
        # Bytes for overhang
        xo = ov_bytes{(xv & xom) >> (8 - wv)}
        xo = selV{xo, xse}
        # Initialize state vectors
        ind  = ind0
        mask = mask0
      } else {
        # Update state vectors
        xo   = shr{V, xo, 1}
        ind  = selV{ind, ind_up} + ind_inc
        mask = advance_spaced_mask{wv, mask, mask_sh}
      }
      b := selV{xv, ind} & mask
      rv:= (b<<wv) - b
      o := xo & W**0xff # overhang
      output{rv | o}
    }
  }
}

export{'si_constrep_bool', rep_const_bool{}}
