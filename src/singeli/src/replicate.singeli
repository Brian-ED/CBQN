include './base'
if_inline (hasarch{'BMI2'}) include './bmi2'
include './mask'
include './spaced'

def ind_types = tup{i8, i16, i32}
def dat_types = tup{...ind_types, u64}

# Indices and Replicate using plus- or max-scan
def scan_core{upd, set, scan, rp:*T, wp:W, s:(usz)} = {
  def getw{j} = if (isptr{W}) cast_i{usz,load{wp,j}} else wp
  b:usz = 1<<10
  k:usz = 0; j:usz = 0; ij:=getw{j}
  while (1) {
    e := tern{b<s-k, k+b, s}
    @for (rp over i from k to e) rp = 0
    if (set) store{rp, k, cast_i{T,j}}
    while (ij<e) { ++j; upd{rp, j, ij}; ij+=getw{j} }
    scan{rp+k, e-k}
    if (e==s) return{}
    k = e
  }
}
def indrep_by_sum{T, rp:*T, wp, s:(usz), js, inc} = {
  def scan{ptr, len} = @for (ptr over len) js=ptr+=js
  def scan{ptr:*E, len if width{T}<=32} = {
    def scanfn = merge{'si_scan_pluswrap_u',fmtnat{width{T}}}
    p := *ty_u{E}~~ptr
    emit{void, scanfn, p, p, len, js}; js=load{ptr,len-1}
  }
  def upd{rp, j, ij} = store{rp, ij, load{rp,ij}+inc{j}}
  scan_core{upd, 0, scan, rp, wp, s}
}

fn ind_by_scan_i32{W}(xv:*void, rp:*i32, s:usz) : void = {
  xp := *W~~xv
  if (hasarch{'X86_64'} and not hasarch{'SSE4.1'}) { # no min instruction
    js:i32 = 0
    indrep_by_sum{i32, rp, xp, s, js, {j}=>1}
  } else {
    scan_core{
      {rp,j,ij} => store{rp,ij,cast_i{i32,j}}, 1,
      {ptr,len} => emit{void, 'si_scan_max_i32', ptr,ptr,len},
      rp, xp, s
    }
  }
}

def rep_by_scan{T, wp, xv:(*void), rv:(*void), s} = {
  xp := *T~~xv; js := *xp; px := js
  def inc{j} = {sx:=px; px=load{xp,j}; px-sx}
  indrep_by_sum{T, *T~~rv, wp, s, js, inc}
}
fn rep_by_scan{W, T}(wp:*void, xv:*void, rv:*void, s:usz) : void = {
  rep_by_scan{T, *W~~wp, xv, rv, s}
}

exportT{'si_indices_scan_i32', each{ind_by_scan_i32, ind_types}}
exportT{'si_replicate_scan', flat_table{rep_by_scan, ind_types, dat_types}}


# Constant replicate
if_inline (not (hasarch{'AVX2'} or hasarch{'AARCH64'})) {

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  rep_by_scan{T, cast_i{usz,wv}, x, r, cast_i{usz, wv*n}}
}

} else {

def incl{a,b} = slice{iota{b+1},a}

# 1+˝∨`⌾⌽0=div|⌜range
def makefact{divisor, range} = {
  def t = table{{a,b}=>0==b%a, divisor, range}
  fold{+, 1, reverse{scan{|, reverse{t}}}}
}
def basic_rep = incl{2, 7}
def fact_size = 128
def fact_inds = slice{iota{fact_size},8}
def fact_tab = makefact{basic_rep, fact_inds}
factors:*u8 = fact_tab



def sdtype = [arch_defvw/8]i8 # shuf data type
def get_shufs{step, wv, len} = {
  def i = iota{len*step}
  split{step, (i - i%wv)/wv}
}
def get_shuf_data{wv, len} = get_shufs{vcount{sdtype}, wv, len} # [len] byte-selector vectors for wv/sdtype (expanded to wider types by read_shuf_vecs)
def get_shuf_data{wv} = get_shuf_data{wv, wv}

# all shuffle vectors for 𝕨≤7
def special_2 = ~hasarch{'AARCH64'} # handle 2 specially on x86-64
def rcsh_vals = slice{basic_rep, special_2}
rcsh_offs:*u8 = shiftright{0, scan{+,rcsh_vals}}
rcsh_data:*i8 = join{join{each{get_shuf_data, rcsh_vals}}}

# first 4 shuffle vectors for 11≤𝕨≤61; only uses the low half of the input
def rcsh4_dom = replicate{bind{>=,64}, replicate{fact_tab==1, fact_inds}}
rcsh4_dat:*i8 = join{join{each{get_shuf_data{., 4}, rcsh4_dom}}}
rcsh4_lkup:*i8 = shiftright{0, scan{+, fold{|, table{==, rcsh4_dom, iota{64}}}}}

def read_shuf_vecs{l, ellw:(u64), shp:*V} = { # tuple of byte selectors in 1<<ellw
  def double{x:X if hasarch{'AVX2'}} = {
    s:=shuf{[4]u64, x, 4b3120}; s+=s
    r:=each{bind{~~,[32]i8},mzip128{s, s + X**1}}
    r
  }
  def double{x:X if hasarch{'AARCH64'}} = {
    s:= x+x
    zip{s, s + X**1}
  }
  def doubles{n,tup} = slice{join{each{double,tup}}, 0, n}
  
  def sh = each{{v}=>{r:=v}, l**V**0}
  def tlen{e} = cdiv{l, e}  # Length for e bytes, rounded up
  def set{i} = { select{sh,i} = each{load{shp,.}, i} }
  def ext{e} = {
    def m = tlen{2*e}; def n = tlen{e}  # m<n
    if (ellw <= lb{e}) set{slice{iota{n},m}}
    else slice{sh,0,n} = doubles{n,slice{sh,0,m}}
  }
  set{iota{tlen{8}}}; ext{4}; ext{2}; ext{1}
  sh
}

def rep_const_shuffle{wv, onreps, xv:*V=[step]T, rv:*V, n:(u64)} = { # onreps{inputVector, {nextOutputVector} => ...}
  nv := n / step
  j:u64 = 0
  def write{v} = { store{rv, j, v}; ++j }
  @for (xv over nv) onreps{xv, write}
  if (nv*step < n) {
    nr := n * wv
    e := nr / step
    s := V**0
    def end = makelabel{}
    onreps{load{xv,nv}, {v} => {
      s = v
      if (j == e) goto{end}
      write{s}
    }}
    setlabel{end}
    q := nr & (step-1)
    if (q!=0) homMaskStoreF{rv+e, maskOf{V, q}, s}
  }
}

if_inline (hasarch{'AVX2'}) {
  def rep_iter_from_sh{sh}{x, gen} = {
    def l = length{sh}
    def h = l>>1
    def fs{v, s} = gen{sel{[16]i8, v, s}}
    a := shuf{[4]u64, x, 4b1010}; each{fs{a,.}, slice{sh,0,h}}
    if (l%2) fs{x, select{sh, h}}
    b := shuf{[4]u64, x, 4b3232}; each{fs{b,.}, slice{sh,-h}}
  }
  
  def get_rep_iter{V, wv==2}{x, gen} = {
    def s = shuf{[4]u64, x, 4b3120}
    each{{q}=>gen{V~~q}, mzip128{s, s}}
  }
  def get_rep_iter{V==[4]u64, wv} = {
    def step = 4
    def sh = each{base{4,.}, get_shufs{step, wv, wv}}
    {x, gen} => each{{s}=>gen{shuf{V, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv, xv:*V, rv:*V, n:(u64)} = rep_const_shuffle{wv, get_rep_iter{V, wv}, xv, rv, n}
  
} else if_inline (hasarch{'AARCH64'}) {
  
  def rep_iter_from_sh{sh}{x, gen} = {
    each{{s} => gen{sel{[16]u8, x, s}}, sh}
  }
  
  def rep_const_shuffle{wv==2, xv0:*V=[_]T, rv0:*V, n:(u64)} = {
    def E = ty_u{T}
    rv:= *E~~rv0
    @for (x in *E~~xv0 over i to n) { # autovectorized well enough, probably
      store{rv, i*2, x}
      store{rv, i*2+1, x}
    }
  }
}

fn rep_const_shuffle_partial4(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  def h = 4
  def V = sdtype
  def sh = read_shuf_vecs{h, ellw, *V~~rcsh4_dat + h*load{rcsh4_lkup,wv}}
  def [step]_ = V          # Bytes written
  def wvb = wv << ellw
  def hs = (h*step) / wvb  # Actual step size in argument elements
  def shufbase{i if hasarch{'AVX2'}} = shuf{[4]u64, load{*V~~(x+i)}, 4b1010}
  def shufbase{i if hasarch{'AARCH64'}} = load{*V~~(x+i)}
  def shufrun{a, s} = sel{[16]i8, a, s} # happens to be the same across AVX2 & NEON
  
  i:u64 = 0
  re := r + n*wvb - h*step
  while (r <= re) {
    a := shufbase{i}
    @unroll (j to h) store{*V~~r, j, shufrun{a, select{sh,j}}}
    i += hs << ellw
    r += hs*wvb
  }
  re+= (h-1)*step
  a:= shufbase{i}
  s:= V**0
  
  def end = makelabel{}
  @unroll (j to h) {
    s = shufrun{a, select{sh,j}}
    if (r > re) goto{end}
    store{*V~~r, 0, s}
    r+= step
  }
  setlabel{end}
  
  q := (re+step) - r
  if (q!=0) homMaskStoreF{*V~~r, maskOf{V, q}, s}
}



fn rcsh_sub{wv, V}(ellw:u64, x:*i8, r:*i8, n:u64, sh:*V) : void = {
  def st = read_shuf_vecs{wv, ellw, sh}
  rep_const_shuffle{wv, rep_iter_from_sh{st}, *V~~x, *V~~r, n}
}

fn rep_const_shuffle_any(wv:u64, ellw:u64, x:*i8, r:*i8, n:u64) : void = {
  if (wv > select{rcsh_vals,-1}) {
    return{rep_const_shuffle_partial4(wv, ellw, x, r, n)}
  }
  n <<= ellw
  ri := wv - select{rcsh_vals,0}
  sh := *sdtype~~rcsh_data + load{rcsh_offs,ri}
  def try{k} = { if (wv==k) rcsh_sub{k, sdtype}(ellw, x, r, n, sh) }
  each{try, rcsh_vals}
}

def rep_const_broadcast{T, kv, loop, wv:(u64), x:*T, r:*T, n:(u64)} = {
  assert{kv > 0}
  def V = [arch_defvw/width{T}]T
  @for (x over n) {
    v := V**x
    @loop (j to kv) store{*V~~r, j, v}
    r += wv
    store{*V~~r, -1, v}
  }
}
fn rep_const_broadcast{T, kv    }(wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, unroll, wv, x, r, n}
fn rep_const_broadcast{T}(kv:u64, wv:u64, x:*T, r:*T, n:u64) : void = rep_const_broadcast{T, kv, for   , wv, x, r, n}

fn rep_const{T}(wv:u64, x:*void, r:*void, n:u64) : void = {
  assert{wv>=2}
  if (wv>=8 and wv<=fact_size) {
    fa := promote{u64, load{factors,wv-8}}
    if (fa > 1) {
      fi := wv / fa
      def t = *void~~(*T~~r + (promote{u64,wv}-fi)*n)
      rep_const{T}(fi,x,t,n)
      rep_const{T}(fa,t,r,fi*n)
      return{}
    }
  }
  def wT = width{T}
  def vn = arch_defvw/wT
  def V = [vn]T
  def max_shuffle = 2*vn
  if (wv <= max_shuffle) {
    def specialize{k} = {
      if (wv==k) return{rep_const_shuffle{k, *V~~x, *V~~r, n}}
    }
    if (special_2) specialize{2}
    rep_const_shuffle_any(wv, lb{wT/8}, *i8~~x, *i8~~r, n)
  } else {
    kv := wv / vn
    @unroll (k from (max_shuffle/vn) to 4) {
      if (kv == k) return{rep_const_broadcast{T, k}(wv, *T~~x, *T~~r, n)}
    }
    rep_const_broadcast{T}(kv, wv, *T~~x, *T~~r, n)
  }
}

}

exportT{'si_constrep', each{rep_const, dat_types}}



# Constant replicate on boolean
fn rep_const_bool{}(wv:usz, x:*u64, r:*u64, rlen:usz) : u1 = {
  def has_pdep = 0  # Obselete, kept here for descriptiveness
  if (wv > 32) return{0}
  m:u64 = spaced_mask_of{wv}
  xw:u64 = 0
  d := cast_i{usz, popc{m}} # == 64/wv
  nw := cdiv{rlen, 64}
  if (wv&1 != 0) {
    rep_const_bool_generic_odd{wv, x, r, nw, m, d}
  } else if (not has_pdep and wv <= 8) {
    return{0}
  } else if (m&1 != 0) {  # Power of two
    i := -usz~~1
    def expand = if (has_pdep) pdep{., m} else {
      mult:u64 = spaced_mask_of{wv-1} >> d
      xm := (u64~~1 << d) - 1
      {xw} => ((xw&xm)*mult) & m
    }
    @for (r over j to nw) {
      xw >>= d
      if ((j&(wv-1))==0) { ++i; xw = load{x, i} }
      rw := expand{xw}
      r = (rw<<wv) - rw
    }
  } else {
    q := cast_i{usz, ctz{m}}  # == 64%wv
    m = m<<(wv-q) | 1
    mt := u64~~1 << (d+1)  # Bit d+1 may be needed, isn't pdep-ed
    tsh := d*wv-(d+1)
    xb := *u8~~x
    xi:usz=0; o:usz=0
    def expand = if (has_pdep) pdep{., m} else {
      {mult, _} := unaligned_spaced_mask_mod{wv-1}
      xm := mt - 1
      {xw} => ((xw&xm)*mult) & m
    }
    @for (r over j to nw) {
      xw = loadu{*u64~~(xb + xi/8)} >> (xi%8)
      ex := (xw & mt) << tsh
      rw := expand{xw}
      r = ((rw-ex)<<(wv-o)) - (rw>>o|(xw&1))
      o += q
      oo := o>=wv; xi+=d+promote{usz,oo}; o-=wv&-oo
    }
  }
  1
}

def rep_const_bool_generic_odd{k, xp, rp, nw, m, d} = {
  # Every-k-bits mask
  mask_sh := cast_i{usz, ctz{m}}  # == 64%k
  mask := m<<(k-mask_sh) | 1

  # Transform sending bit i to k*i % 64 by pairwise swaps
  # Swap data goes in a pre-computed table
  def swdat{lw, k} = {
    def i = iota{lw}
    def bits = (k*iota{1<<lw} >> merge{0, replicate{1<<i, i}}) & 1
    ~u64~~base{2, bits}
  }
  swtab:*u64 = each{swdat{6,.}, 1+2*iota{16}}
  def swap_lens = reverse{2 << iota{5}}
  swap_data := load{swtab, k>>1}
  swsel:u64 = ~u64~~0
  def gsw{l} = {
    swsel ^= swsel << l  # Low l bits out of every 2*l
    sm := swap_data &~ swsel
    swap_data &= swsel; swap_data |= swap_data<<l
    sm
  }
  swap_masks := each{gsw, swap_lens}
  i:usz = 0
  def get_swap_x{} = {
    # Load x, send bit i to position k*i % 64
    x := load{xp, i}; ++i
    def swap_step{l, m} = {
      xx := (x ^ x<<l) & m
      x ^= xx | (xx>>l)
    }
    def swap_step{l==32, m} = { # Use rotate
      mm := m | m>>l
      x = (x &~ mm) | ((x<<l | x>>l) & mm)
    }
    each{swap_step, swap_lens, swap_masks}
    x
  }

  # Output
  j:usz = 0
  def output{rw} = {
    store{rp, j, rw}
    ++j; if (j==nw) return{1}
  }
  o:u64 = 0  # carry
  # Dedicated loop for 3, shared for other factors
  if (k == 3) {
    while (1) {
      x := get_swap_x{}
      @unroll (jj to 3) {
        b := x & mask
        mask = (mask << (3 - 64%3)) | (mask >> (64%3))
        def os = (64-3) + 1 + iota{select{tup{0,2,1}, jj}}
        output{fold{|, (b<<3) - b, each{>>{o,.}, os}}}
        o = b
      }
    }
  } else {
    # Fundamental operation: shifts act as order-k cyclic group on masks
    def advance{m, sh} = m<<(k-sh) | m>>sh
    sm0 := mask    # starting mask
    s1 := mask_sh  # single iteration shift
    # Get cumulative mask for 4 iterations, and shift to advance 4
    def double{{mc, s}} = {
      def ss = s+s
      tup{mc|advance{mc,s}, ss - (k &- (ss>k))}
    }
    {mc4, s4} := double{double{tup{sm0, s1}}}
    # Submasks pick one mask out of a combination of 4
    def or_adv{m, s} = { m |= advance{m,s} }
    @for (min{k/4 - 1, nw/4}) or_adv{sm0,s4}
    submasks := scan{advance, tup{sm0, ...3**s1}}
    mask_tail := advance{sm0, s4} &~ sm0
    # Mask out carry bit
    mr := u64~~1<<k - 1
    while (1) {
      x := get_swap_x{}
      os:=o; xo:=x<<k|x>>(64-k); o=xo&1; xo=(xo&~1)|os
      mask = mc4
      # Write result word given starting bits
      def step{b, c} = output{c - b - promote{u64, c&mr != 0}}
      def step{b, c, m} = step{b&m, c&m}
      # Fast unrolled iterations
      @for (k/4) {
        each{step{x & mask, xo & mask, .}, submasks}
        mask = advance{mask, s4}
      }
      # Single-step for tail
      mask = mask_tail
      @for (k%4) {
        step{x, xo, mask}
        mask = advance{mask, s1}
      }
    }
  }
}

fn rep_const_bool{if hasarch{'SSSE3'}}(wv:usz, x:*u64, r:*u64, rlen:usz) : u1 = {
  if (wv > 32) return{0}
  if (wv&1 == 0) {
    p := ctz{wv | 8} # Power of two for second replicate
    if (wv>>p == 1) {
      rep_const_bool_ssse3_div8{wv, x, r, rlen}
    } else {
      tlen := rlen>>p
      t := r + cdiv{rlen, 64} - cdiv{tlen, 64}
      rep_const_bool{}(wv    >>p, x, t, tlen)
      rep_const_bool{}(usz~~1<<p, t, r, rlen)
    }
  } else {
    rep_const_bool_ssse3_odd{wv, x, r, rlen}
  }
  1
}

# Generalized flat transpose of iota{1<<length{bs}}
# select{tr_iota{bs}, x} sends bit i of x to position select{bs, i}
def tr_iota{...bs} = {
  def axes = each{tup{0,.}, 1<<bs}
  fold{flat_table{|,...}, reverse{axes}}
}
def tr_iota{{...bs}} = tr_iota{...bs}

def get_boolvec_writer{V, r, rlen} = {
  def vwords = width{V}/64
  nw := cdiv{rlen, 64}
  rv := *V~~r
  re := rv + nw / vwords
  last_res:V = V**0
  def end = makelabel{}
  def output{v:(V)} = {
    last_res = v
    if (rv==re) goto{end}
    store{rv, 0, v}; ++rv
  }
  def flush{} = {
    setlabel{end}
    q := nw & (vwords-1)
    if (q != 0) homMaskStoreF{rv, V~~maskOf{re_el{u64,V}, q}, last_res}
  }
  tup{output, flush}
}

def rep_const_bool_ssse3_div8{wv, x, r, rlen} = { # wv in 2,4,8
  oper // ({a,b}=>floor{a/b}) infix left  40
  def avx2 = hasarch{'AVX2'}
  def vl = if (avx2) 32 else 16
  def V = [vl]u8
  def iV = iota{vl}
  def mkV = make{V, .}
  def selH = sel{[16]u8, ., .}
  def makeTab{t} = selH{mkV{if (avx2) merge{t,t} else t}, .}
  def id{xv} = xv
  def {output, flush} = get_boolvec_writer{V, r, rlen}

  def run24{x, proc_xv, exh} = {
    i:usz = 0; while (1) {
      xv := proc_xv{load{*V~~(x+i)}}; ++i
      # Store 1 or 2 result vectors
      def getr = zip128{exh{xv}, exh{V~~(re_el{u16,V}~~xv>>4)}, .}
      output{V~~getr{0}}
      output{V~~getr{1}}
    }
  }
  if (wv == 2) {
    def init = if (avx2) shuf{[4]u64, ., 4b3120} else id
    # Expander for half byte
    def tabr = makeTab{tr_iota{2*iota{4}} * 2b11}
    m4 := V**0xf
    run24{*V~~x, init, {x} => tabr{x & m4}}
  } else if (wv == 4) {
    # Unzip 32-bit elements (result lanes) across AVX2 lanes
    def pre = if (avx2) sel{[8]u32, ., make{[8]u32,tr_iota{1,2,0}}} else id
    def init{xv} = { u:=pre{xv}; zip128{u,u,0} }
    # Expander for two bits in either bottom or next-to-bottom position
    def tabr = makeTab{tr_iota{0,4,0,4} * 2b1111}
    m2 := mkV{2b11 << (2*(iV%2))}
    def exh{x} = re_el{u16, V}~~tabr{x & m2}
    run24{*(if (avx2) [2]u64 else u64)~~x, init, exh}
  } else { # wv == 8
    i:usz = 0; while (1) {
      xh := load{*[16]u8~~(*ty_u{vl}~~x + i)}; ++i
      xv := if (avx2) pair{xh, xh} else xh
      xe := selH{xv, mkV{iV // 8}}
      output{(xe & mkV{1 << (iV % 8)}) > V**0}
    }
  }
  flush{}
}

# For odd numbers:
# - permute each byte sending bit i to position k*i % 8
# - replicate each byte by k, making position k*i contain bit i
# - mask out those bits and spread over [ k*i, k*(i+1) )
# - ...except where it crosses words; handle this overhang separately
def rep_const_bool_ssse3_odd{wv, x, r, rlen} = { # wv odd, wv<=15
  oper // ({a,b}=>floor{a/b}) infix left  40
  def vl = 16; def V = [vl]u8
  def iV = iota{vl}
  def mkV = make{V, .}; def selV = sel{V, ., .}
  def W = [2]u64
  def {output, flush} = get_boolvec_writer{V, r, rlen}

  # Within-byte transformation
  def get_ttab{k} = each{{is} => mkV{tr_iota{is}}, split{4, k*iota{8} % 8}}
  ttab:*V = join{each{get_ttab, 2*iota{4} + 1}}
  {t0, t4} := each{load{ttab + (wv & 6), .}, iota{2}}
  m4 := V**0xf
  def perm_x{xv} = {
    selV{t0, xv & m4} | selV{t4, V~~([8]u16~~xv>>4) & m4}
  }

  # Cases are 3; 5 7; 9 11 13 15
  if (wv < 4) {
    # 3: dedicated loop
    i:usz = 0; while (1) {
      # 01234567 to 05316427 on each byte
      xv := perm_x{load{*V~~x, i}}; ++i
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{3},1} // 3  # bits that overhang within a word
      def ib = ix // 8                   # byte index
      def io = 8*ib + 3*ix%8             # where they are in xv
      def wi = split{2, tup{255, ...ib, 255, ...8+ib}}
      xo := V~~((W~~xv & W**fold{|, 1<<io}) >> (8-3))
      xo += xo > V**0
      # Permute and mask bytes
      def step{jj, oi, ind, mask} = {
        b := W~~(selV{xv, ind} & mask)
        r := V~~((b<<3) - b)
        o := selV{xo, mkV{flat_table{max, oi, 255*(0<iota{8})}}}
        output{r|o}
      }
      def make3V{vs} = each{make{V,.}, split{vl, vs}}
      each{step,
        iota{3}, wi,
        make3V{replicate{3, iota{vl}}},
        make3V{8w2b001 << ((-8)*iota{3*vl} % 3)}
      }
    }
  } else if (wv < 8) {
    # 5, 7: precompute constants, then shared loop
    {xom, xse, ind0, mask0, ind_up, ind_inc, mask_sh} := undef{tup{
     W,   V,   V,    V,     V,      V,       usz   }}
    def set_consts{k} = {
      # Overhang from previous 64-bit elements
      def ix = 64*slice{iota{k},1} // k  # bits that overhang within a word
      def ib = ix // 8                   # byte index
      def io = 8*ib + k*ix%8             # where they are in xv
      def wi = tup{255, ...ib, 255, ...8+ib}
      xom = W**fold{|, 1<<io}
      xse = mkV{join{flip{split{2, shiftright{wi, vl**255}}}}}
      # Permutation to expand by k bytes, and every-k-bits mask
      ind0  = mkV{iV // k}
      mask0 = mkV{((1<<k|1) << ((-8)*iV % k)) % 256}
      def iu = iV + vl%k; def ia = iu>=vl
      ind_up  = mkV{iu - k*ia}
      ind_inc = mkV{vl//k + ia}
      mask_sh = width{V} % k
    }
    if (wv == 5) set_consts{5} else set_consts{7}
    xv:V = V**0; xo:=xv; ind:=xv; mask:=xv  # state
    i:usz = 0; q:usz = 1
    while (1) {
      --q; if (q == 0) { q = wv
        # Load and permute bytes
        xv = perm_x{load{*V~~x, i}}; ++i
        # Bytes for overhang
        xo = V~~((W~~xv & xom) >> (8 - wv))
        xo += xo > V**0
        xo = selV{xo, xse}
        # Initialize state vectors
        ind  = ind0
        mask = mask0
      } else {
        # Update state vectors
        xo   = shr{V, xo, 1}
        ind  = selV{ind, ind_up} + ind_inc
        mask = V~~((W~~mask << (wv - mask_sh)) | (W~~mask >> mask_sh))
      }
      b := W~~(selV{xv, ind} & mask)
      rv:= V~~((b<<wv) - b)
      o := xo & mkV{255 * (iV%8 == 0)} # overhang
      output{rv | o}
    }
  } else { # wv < 32
    # 9 to 31: extend k*i % 8 transform to k*i % 128 by pairwise swaps
    # Swap data goes in a pre-computed table
    def swdat{k} = {
      def bits = (k*iota{128} >> merge{0, replicate{1<<iota{7}, iota{7}}}) & 1
      mkV{each{base{2, .}, split{8, bits}}}
    }
    swtab:*V = each{swdat, 9+2*iota{12}}
    def swap_lens = reverse{1 << iota{4}}
    swap_data := load{swtab, (wv>>1)-4}
    swap_masks := each{{l} => selV{swap_data, mkV{l+iV%l}}, swap_lens}
    # Every-k-bits mask, same as before
    {m, d} := unaligned_spaced_mask_mod{wv}
    mask := make{W, m, m>>d|m<<(wv-d)}
    mask_sh := d+d; if (mask_sh >= wv) mask_sh-= wv
    # Mask out carry bit
    mr := [4]u32~~W**(u64~~1<<wv - 1)
    # State
    xv:V = V**0; o:=W**0
    i:usz = 0
    while (1) {
      # Load xv, send bit i to position wv*i % 128
      xv = load{*V~~x, i}; ++i
      def swap_step{l, m} = {
        xv = (xv & m) | (selV{xv, mkV{iV + (l-2*(iV&l))}} &~ m)
      }
      each{swap_step, swap_lens, swap_masks}
      xv = perm_x{xv}
      xw := W~~xv
      def vrot1{x} = vshl{x, x, vcount{type{x}}-1}
      w1 := W**1
      os:=o; xo:=xw<<wv|vrot1{xw>>(64-wv)}; o=xo&w1; xo=(xo&~w1)|os
      # Write wv vectors based on that
      @for (wv) {
        b := xw & mask
        # Handle overhang here; won't fit in a single vector
        c := xo & mask; cu := [4]u32~~c
        output{V~~(W~~(cu + (mr&cu > [4]u32**0)) - b)}
        mask = (mask << (wv - mask_sh)) | (mask >> mask_sh)
      }
    }
  }
  flush{}
}
export{'si_constrep_bool', rep_const_bool{}}
