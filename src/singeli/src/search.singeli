include './base'
if (hasarch{'AVX2'}) {
  include './sse'
  include './avx'
  include './avx2'
} else if (hasarch{'SSSE3'}) {
  include './sse'
} else if (hasarch{'X86_64'}) {
  include './sse2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './mask'
include './vecfold'

def ctzi{x} = promote{u64, ctz{x}}  # Count trailing zeros, as index

def findFirst{C, M, F, ...v1} = {
  def exit = makelabel{}
  def args = undef{M{...each{{c}=>tupsel{0,c}, v1}}}
  def am = tuplen{tupsel{0,v1}}
  each{{last, ...v2} => {
    if (last or C{...v2}) {
      each{=, args, M{...v2}}
      goto{exit}
    }
  }, iota{am} == am-1, ...v1}
  unreachable{}
  setlabel{exit}
  F{...args}
}

def search{E, x, n:u64, OP} = {
  def bulk = arch_defvw/width{E}
  def VT = [bulk]E
  def end = makeBranch{
    tup{u64, ty_u{VT}},
    {i,c} => return{i*bulk + ctzi{homMask{c}}}
  }
  
  muLoop{bulk, tern{arch_defvw>=256, 1, 2}, n, {is, M} => {
    eq:= each{OP, loadBatch{*E~~x, is, VT}}
    if (homAny{M{tree_fold{|, eq}}}) {
      findFirst{
        {i,c} => homAny{c},
        {i,c} => tup{i,c},
        end,
        is, eq
      }
    }
  }}
  n
}

fn searchOne{A, E}(x:*void, e0:A, len:u64) : u64 = {
  def e = if (A==E) e0 else cast_i{E, e0}
  search{E, x, len, {c:VT} => c == VT**e}
}

def isNegZero{x:T} = to_el{u64,x} == to_el{u64, T ** -f64~~0}
fn searchNormalizable{}(x:*f64, len:u64) : u64 = {
  search{f64, x, len, {c:VT} => isNegZero{c} | (c!=c)}
}

fn copyOrdered{}(r:*f64, x:*f64, len:u64) : u1 = {
  def E = f64
  def bulk = arch_defvw/width{E}
  def VT = [bulk]E
  maskedLoop{bulk, len, {i, M} => {
    c:= loadBatch{x, i, VT}
    if (homAny{M{c!=c}}) return{1}
    storeBatch{r, i, c + VT**0, M}
  }}
  0
}

if (hasarch{'X86_64'} | hasarch{'AARCH64'}) {
  export{'simd_search_u8',  searchOne{u64, u8}}
  export{'simd_search_u16', searchOne{u64, u16}}
  export{'simd_search_u32', searchOne{u64, u32}}
  export{'simd_search_f64', searchOne{f64, f64}}
  export{'simd_search_normalizable', searchNormalizable{}}
  export{'simd_copy_ordered', copyOrdered{}}
}


# In-register bit table
def arch_vec{T} = [arch_defvw/width{T}]T
def TI = i8     # Table values
def VI = arch_vec{TI}
def simd_bittab = hasarch{'SSSE3'}

def bittab_init{tab, z} = {
  @for (t in *TI~~tab over 256) t = z
}
def bittab_init{tab, z & simd_bittab} = {
  init:= VI**z
  @unroll (t in *VI~~tab over 256/vcount{VI}) t = init
}

def bittab_selector{loadtab} = {
  def nv = vcount{VI}
  {t0, t1}:= loadtab{}
  low:= VI**7
  hi4:= VI**(-(1<<4))
  b  := VI~~make{[nv]u8, 1 << (iota{nv} & 7)}
  def selector{x} = {
    top := hi4 + VI~~((arch_vec{u32}~~(x&~low))>>3)
    byte:= sel{[16]i8, t0, hi4^top} | sel{[16]i8, t1, top}
    mask:= sel{[16]i8, b, x & low}
    homMask{(mask & byte) == mask}
  }
  def reload{} = { tup{t0,t1} = loadtab{} }
  tup{selector, reload}
}

def readbytes{vtab}{} = {
  def k = vcount{VI}; def l = 128/vcount{VI}
  def side{i} = {
    def U = arch_vec{ty_u{k}}
    def m = @collect (vtab over _ from i to i+l) homMask{vtab}
    VI~~make{U, if (vcount{U}>l) merge{m,m} else m}
  }
  each{side, l*iota{2}}
}

# Look up bits from table
def bittab_lookup{x0:*void, n:u64, r0:*void, tab:*void} = {
  x:= *u8~~x0
  t:= *TI~~tab
  r:= *u64~~r0
  rem:= n
  while (rem > 0) {
    k:= rem; if (k>64) k=64
    rv:u64 = 0
    @for (x over j to k) rv|= u64~~promote{i64,load{t,x}} & ((u64~~1)<<j)
    store{r, 0, rv}
    x+=k; rem-=k; ++r
  }
}
def bittab_lookup{x0:*void, n:u64, r0:*void, tab:*void & simd_bittab} = {
  def {bitsel, _} = bittab_selector{readbytes{*VI~~tab}}
  def k = vcount{VI}
  @for (x in *VI~~x0, r in *ty_u{k}~~r0 over cdiv{n,k}) r = bitsel{x}
}

# Fill table with t (0 or -1) at all bytes in x0
# Stop early if the sum u reaches 0, indicating all bytes in the table
# are equal: by the time it's checked at least one has been set to t,
# so they're all t
# Fill r0 depending on mode:
# - 'none':   ignore
# - 'mask':   Mark Firsts of x0
# - 'unique': Deduplicate of x0
# - 'index':  First index of value x at r0+x
def do_bittab{x0:*void, n:u64, tab:*void, u:u8, t, mode, r0} = {
  def rbit = mode == 'mask'
  def rval = mode == 'unique'
  def rind = mode == 'index'
  def storebit{i, v:T} = if (rbit) store{*T~~r0, i, v}
  if (rbit or rval) assert{match{t,0}}

  btab:= *i8~~tab
  def settab_sub{x, v, i} = {
    if (rval) store{*u8~~r0, u, x}
    if (rind and v!=t) store{r0, x, i}
    u+= u8~~i8~~(t - v)  # u tracks the total of btab
    store{btab, x, t}
    v
  }
  def settab1{x, i} = settab_sub{x, -1 - t, i}       # Known new
  def settab{x, i} = settab_sub{x, load{btab, x}, i} # General case
  def settab{T, x, i} = T~~promote{ty_s{T}, settab{x, i}}

  x:= *u8~~x0
  if (not simd_bittab) {
    rem:= n
    @for (i to cdiv{n,64}) {
      k:= rem; if (k>64) k=64
      rw:u64 = 0
      @for (x over j to k) {
        new:= settab{u64, x, i*64+j} # Index usually unused
        if (rbit) rw|= new & ((u64~~1)<<j)
      }
      storebit{i, rw}
      x+=k; rem-=k
    }
  } else {
    # Do first few values with a scalar loop
    # Avoids the cost of ever loading the table into vectors for n<=48
    i:u64 = 32; if (n<=48) i=n
    def k = vcount{VI}; def uk = ty_u{k}; def ik = ty_s{k}
    {rw,rv} := undef{tup{u64,uk}} # Bit results, used if rbit
    if (rbit) rw = 0
    @for (x over j to i) {
      new:= settab{u64, x, j}
      if (rbit) rw|= new & ((u64~~1)<<j)
    }
    storebit{0, rw}
    if ((mode == 'none' or rind) and u == 0) return{u} # Won't ever trigger (m != 0)!

    def done = makelabel{}
    def {bitsel, reload_tab} = bittab_selector{readbytes{*VI~~tab}}
    xv:= *VI~~x0
    while (i < n) {
      i0:= i; iw:= i/k
      v:= load{xv, iw}
      m:= bitsel{v} # Mask of possibly-new values
      if (not match{t,0}) m^= uk~~promote{ik, t}
      i+= k
      if (i > n) m&= (~uk~~0)>>((-n)%k)
      # Any new values?
      if (m == 0) {
        storebit{iw, m}
      } else {
        # Add values to the table and filter m
        if (rbit) rv = m
        im:= i0 + ctzi{m}
        xi:= load{x, im}
        settab1{xi, im}
        if ((m&(m-1)) != 0) { # More bits than one
          # Filter out values equal to the previous, or first new
          def pind = (iota{k}&15) - 1
          prev:= make{VI, each{bind{max,0}, pind}}
          e:= ~homMask{v == VI**TI~~xi}
          e&= base{2,pind<0} | ~homMask{v == sel{[16]i8, v, prev}}
          if (rbit) rv&= e | -m # Don't remove first bit
          m&= e
          while (m != 0) {
            im:= i0 + ctzi{m}
            new:= settab{uk, load{x, im}, im}
            m1:= m-1;  m&= m1         # Clear low bit
            if (rbit) rv&= m1 | new   # Clear if not new
          }
        }
        storebit{iw, rv}
        if (u == 0) { # All bytes seen
          if (rbit) @for (r in *uk~~r0 over _ from iw+1 to cdiv{n,k}) r = 0
          goto{done}
        }
        reload_tab{}
      }
    }
    setlabel{done}
  }
  u
}

fn simd_mark_firsts_u8(x0:*void, n:u64, r0:*void, tab:*void) : void = {
  bittab_init{tab, -1}
  u:u8 = 0
  do_bittab{x0, n, tab, u, 0, 'mask', r0}
}

fn simd_deduplicate_u8(x0:*void, n:u64, r0:*void, tab:*void) : u64 = {
  assert{n != 0}
  bittab_init{tab, -1}
  u:u8 = 0
  do_bittab{x0, n, tab, u, 0, 'unique', r0}
  1 + promote{u64, u-1}  # 0 to 256
}

fn fill_bittab(x0:*void, n:u64, tab:*void, u:u8, t:i8) : u8 = {
  do_bittab{x0, n, tab, u, t, 'none', 0}
}

fn simd_member_u8(w0:*void, nw:u64, x0:*void, nx:u64, r0:*void, tab:*void) : void = {
  assert{nw > 0}

  rev:u1 = nx < nw/4  # Reverse lookup
  bittab_init{tab, -promote{i8,rev}}
  u:u8 = 0  # Sum of table, either 0 or 256
  if (rev) u = fill_bittab(x0, nx, tab, u, 0)

  u = fill_bittab(w0, nw, tab, u, -1)

  if (u == 0) { # All found!
    @for (r in *u64~~r0 over cdiv{nx,64}) r = maxvalue{u64}
  } else {
    bittab_lookup{x0, nx, r0, tab}
  }
}

fn simd_index_tab_u8{I}(w0:*void, nw:u64, x0:*void, nx:u64, tab:*void, i0:*void) : void = {
  rev:u1 = nx < nw/4
  bittab_init{tab, -promote{i8,rev}}
  ind:= *I~~i0
  @for (ind over 256) ind = cast_i{I, nw}
  u:u8 = 0
  if (rev) u = fill_bittab(x0, nx, tab, u, 0)
  do_bittab{w0, nw, tab, u, -1, 'index', ind}
}

export{'simd_mark_firsts_u8', simd_mark_firsts_u8}
export{'simd_deduplicate_u8', simd_deduplicate_u8}
export{'simd_member_u8', simd_member_u8}
export{'simd_index_tab_u8', simd_index_tab_u8{usz}}

def acc{unr, init:T} = {
  a0v := init
  def a0 = tup{a0v}
  def a1 = @collect(unr) { reg:=init }
  def op{S=='get'} = a0v
  def op{S=='tr', F} = { a0v = tree_fold{F, a1} }
  def op{S=='upd', is, F} = {
    if (tuplen{is}==1) a0 = F{a0}
    else               a1 = F{a1}
  }
}
def isI64{x:T & eltype{T}==f64 & hasarch{'AARCH64'}} = x == cvt{f64, cvt{i64, x}}
def isI64{x:T & eltype{T}==f64 & hasarch{'SSE4.1'}} = (x==floor{x}) & (abs{x}<=T**(1<<53))

def maskBlend{b:T, x:T, M} = x
def maskBlend{b:T, x:T, M & M{0}} = homBlend{b, x, M{T, 'to homogeneous bits'}}

include './debug'
fn getRange{E}(x0:*void, res:*i64, n:u64) : u1 = {
  assert{n>0}
  x:= *E~~x0
  min1:E = *x
  max1:E = *x
  if (has_simd and not (E==f64 and not (hasarch{'AARCH64'} or hasarch{'SSE4.1'}))) {
    def bulk = arch_defvw/width{E}
    def VT = [bulk]E
    def unr = tern{(E==f64) & hasarch{'X86_64'}, 1, 2}
    def minA = acc{2, VT**min1}
    def maxA = acc{2, VT**min1}
    muLoop{bulk, unr, n, {is, M} => {
      def cx = loadBatch{x, is, VT}
      if (E==f64 and homAny{M{tree_fold{|, each{{c} => ~isI64{c}, cx}}}}) return{0}
      minA{'upd', is, {a} => eachx{maskBlend, a, each{min, a, cx}, M}} # blend
      maxA{'upd', is, {a} => eachx{maskBlend, a, each{max, a, cx}, M}} # blend
    }, {} => { minA{'tr',min}; maxA{'tr',max} }}
    min1 = vfold{min, minA{'get'}}
    max1 = vfold{max, maxA{'get'}}
  } else {
    @for (x over i to n) {
      if (E==f64 and rare{x != emit{f64, '', emit{i64, '', x}}}) return{0}
      min1 = min{min1, x}
      max1 = max{max1, x}
    }
  }
  store{res, 0, min1}
  store{res, 1, max1}
  1
}

exportT{'simd_getRangeRaw', each{getRange, tup{i8,i16,i32,f64}}}
