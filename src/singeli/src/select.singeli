include './base'
include './cbqnDefs'
include './sse3'
include './avx'
include './avx2'
include './mask'
include 'util/tup'

# def:T - masked original content
# b:B - pointer to data to index; if width{B}<width{eltype{T}}, padding bytes are garbage read after wanted position
# idx - actual (unscaled) index list
def gather{def:T, b:B, idx:[8]i32, M & w256{T,32}} = {
  if (M{0}) T ~~ emit{[8]i32, '_mm256_mask_i32gather_epi32', def, *i32~~b, idx, M{T,'to sign bits'}, width{eltype{B}}/8}
  else      T ~~ emit{[8]i32, '_mm256_i32gather_epi32',           *i32~~b, idx,                      width{eltype{B}}/8}
}
def gather{def:T, b:B, idx:[4]i32, M & w256{T,64}} = {
  if (M{0}) T ~~ emit{[4]i64, '_mm256_mask_i32gather_epi64', def, *i64~~b, idx, M{T,'to sign bits'}, width{eltype{B}}/8}
  else      T ~~ emit{[4]i64, '_mm256_i32gather_epi64',           *i64~~b, idx,                      width{eltype{B}}/8}
}

select{rw, TI, TD}(w0:*void, x0:*void, r0:*void, wl:u64, xl:u64) : u1 = {
  def TIE = i32
  def TDE = tern{width{TD}<32, u32, TD}
  def bulk = rw / width{TDE}
  def VI = [bulk]TIE
  def VD = [bulk]TDE
  def xlf = broadcast{VI, cast_i{TIE, xl}}
  
  w:= *TI ~~ w0
  x:= *TD ~~ x0
  r:= *TD ~~ r0
  
  maskedLoop{bulk, wl, {i, M} => {
    cw0:= loadBatch{w, i, VI}
    cw1:= cw0+xlf
    cw:= blendF{cw0, cw1, cw0<broadcast{VI, 0}} # TODO this is utilizing clang optimizing out the comparison
    if (any{M{ty_u{cw} >= ty_u{xlf}}}) return{0}
    got:= gather{broadcast{VD,0}, x, cw, M}
    if (TDE!=TD) got&= broadcast{VD, (1<<width{TD})-1}
    storeBatch{r, i, got, M}
  }}
  1
}
def select{TI, TD} = select{256, TI, TD}

def selects = join{table{select, tup{i8, i16, i32},       # indices
                                 tup{u8, u16, u32, u64}}} # values
avx2_select_tab:*type{tupsel{0,selects}} = selects
'avx2_select_tab' = avx2_select_tab
