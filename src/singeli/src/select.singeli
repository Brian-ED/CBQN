include './base'
include './cbqnDefs'
include './sse3'
include './avx'
include './avx2'
include './mask'

def min{a, b & knum{a} & knum{b}} = tern{a<b, a, b}

def minBulk{w, A, B & width{A}< width{B}} = w/width{B}
def minBulk{w, A, B & width{A}>=width{B}} = w/width{A}

# def:T - masked original content
# b:B - pointer to data to index; if width{B}<width{eltype{T}}, padding bytes are garbage read after wanted position
# idx - actual (unscaled) index list
def gather{def:T, b:B, idx:[8]i32, M & w256{T,32}} = {
  if (M{0}) T ~~ emit{[8]i32, '_mm256_mask_i32gather_epi32', def, *i32~~b, idx, M{T,'to sign bits'}, width{eltype{B}}/8}
  else      T ~~ emit{[8]i32, '_mm256_i32gather_epi32',           *i32~~b, idx,                      width{eltype{B}}/8}
}
def gather{def:T, b:B, idx:[4]i32, M & w256{T,64}} = {
  if (M{0}) T ~~ emit{[4]i64, '_mm256_mask_i32gather_epi64', def, *i64~~b, idx, M{T,'to sign bits'}, width{eltype{B}}/8}
  else      T ~~ emit{[4]i64, '_mm256_i32gather_epi64',           *i64~~b, idx,                      width{eltype{B}}/8}
}


select{rw, TI, TD}(w0:*u8, x0:*u8, r0:*u8, wl:u64, xl:u64) : u1 = {
  def TIE = i32
  def TDE = tern{width{TD}<32, u32, TD}
  def bulk = minBulk{rw,TIE,TDE}
  def TIF = [bulk]TIE
  def TDF = [bulk]TDE
  def xlf = broadcast{TIF, cast_i{eltype{TIF}, xl}}
  
  w:= *TI ~~ w0
  x:= *TD ~~ x0
  r:= *TD ~~ r0
  
  maskedLoop{bulk, wl, {i, M} => {
    cw0:= loadBatch{w, i, TIF}
    cw1:= cw0+xlf
    cw:= blendF{cw0, cw1, cw0<broadcast{TIF, 0}} # TODO this is utilizing clang optimizing out the comparison
    if (any{M{ty_vu{cw} >= ty_vu{xlf}}}) return{0}
    got:= gather{broadcast{TDF,0}, x, cw, M}
    if (TDE!=TD) got&= broadcast{TDF, (1<<width{TD})-1}
    storeBatch{r, i, got, M}
  }}
  1
}


'avx2_select_i32_8' = select{256, i32, u8}
'avx2_select_i32_16' = select{256, i32, u16}
'avx2_select_i32_32' = select{256, i32, u32}
'avx2_select_i32_64' = select{256, i32, u64}
'avx2_select_i8_32' = select{256, i8, u32}
'avx2_select_i16_32' = select{256, i16, u32}

