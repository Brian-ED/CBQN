include './base'
if (hasarch{'BMI2'}) {
  include './bmi2'
}
if (hasarch{'PCLMUL'}) {
  include './sse'  # PCLMUL implies SSE4.2
  def clmul{a:T, b:T, imm & w128i{T}} = emit{T, '_mm_clmulepi64_si128', a, b, imm}
} else {
  def clmul{...x} = assert{'clmul not supported', show{...x}}
}
include 'util/tup'

def storeu{p:T, i, v:eltype{T} & *u64==T} = emit{void, 'storeu_u64', p+i, v}
def loadu{p:T & *u64==T} = emit{eltype{T}, 'loadu_u64', p}

def comp8{w:*u64, X, r:*i8, l:u64} = {
  @for(w in *u8~~w over i to cdiv{l,8}) {
    pc:= popc{w}
    storeu{*u64~~r, 0, pext{promote{u64,X{}}, pdep{promote{u64, w}, cast{u64,0x0101010101010101}}*255}}
    r+= pc
  }
}

def tab{n,l} = {
  def m=n-1; def t=tab{m,l}
  def k = (1<<l - 1) << (m*l)
  merge{t, k+t}
}
def tab{n==0,l} = tup{0}
c16lut:*u64 = tab{4,16}

def vgLoad{p:T, i & T == *u64} = emit{eltype{T}, 'vg_loadLUT64', p, i}

def comp16{w:*u64, X, r:*i16, l:u64} = {
  @for(w in *u8~~w over i to cdiv{l,8}) {
    def step{w} = {
      pc:= popcRand{w}
      storeu{*u64~~r, 0, pext{promote{u64,X{}}, vgLoad{c16lut, w}}}
      r+= pc
    }
    step{w&15}
    step{w>>4} # this runs even if the above step was all that's required, so it'll act on the invalid result of "r+= pc", so we need to overallocate even more to compensate
  }
}

fn slash2{F, T}(w:*u64, x:*T, r:*T, l:u64) : void = {
  xv:= *u64~~x
  F{w, {} => {c:= loadu{xv}; xv+= 1; c}, r, l}
}

fn slash1{F, T, iota, add}(w:*u64, r:*T, l:u64) : void = {
  x:u64 = iota
  F{w, {} => {c:= x; x+= add; c}, r, l}
}

# 8-bit writes ~8 bytes of garbage past end, 16-bit writes ~16 bytes
if (hasarch{'BMI2'}) {
export{'bmipopc_2slash8', slash2{comp8, i8}}
export{'bmipopc_2slash16', slash2{comp16, i16}}
export{'bmipopc_1slash8', slash1{comp8, i8, 0x0706050403020100, 0x0808080808080808}}
export{'bmipopc_1slash16', slash1{comp16, i16, 0x0003000200010000, 0x0004000400040004}}
}

# pext, or boolean compress
fn pext{T}(x:T, m:T) {
  def w = width{T}
  def mod{a} = a % (1<<w)
  def lowbits{k} = base{1<<k, cdiv{w,k}**1}
  # At each step, x and z are split into groups of length k
  # - z tells how many bits in the group are NOT used
  # - x contains the bits, with z zeros above
  def build{k==1} = tup{x&m, ~m}
  def build{k & k > 1} = {
    def h = k>>1          # Increase size from h to k
    {x,z} := build{h}
    def low = lowbits{k}  # Low bit in each new group
    if (k <= 3) {
      z0 := z & low
      zm := z>>1 & low
      if (k == 2) tup{
        x - (x>>1 & z0),
        z0 + zm
      } else tup{ # Faster 1->3 jump, currently unused
        x - ((x>>1&mod{low*3}) & (z|z0<<1)) - (x>>2 & (z & zm)),
        (z0 + zm) + (z>>2 & low)
      }
    } else {
      # Shift high x group down by low z, then add halves of z
      even:T = mod{low*(1<<h - 1)}
      # SWAR shifter: shift x by sh*o, in length-k groups
      def shift{sh, o, x} = {
        l := o & low; m := l<<k - l
        s := (x & m)>>sh | (x &~ m)
        if (2*sh<=k/2) shift{2*sh, o>>1, s} else s
      }
      tup{
        (x&even) | shift{1, z, x&~even},
        if (k>4) (z + z>>h)&even else ((z&~even)>>h) + (z&even)
      }
    }
  }
  # Finally, compose groups with regular shifts
  def g = 8  # 12 performs about the same
  {b,z} := build{g}
  o := z*lowbits{g}  # Offsets by prefix sum
  def s = 1<<g - 1
  def gr{sh} = (b & mod{s<<sh}) >> (o>>(sh-g) & s)
  fold{|, b&s, each{gr, g*slice{iota{cdiv{w,g}},1}}}
}

fn pext{T & hasarch{'PCLMUL'} & T==u64}(xs:T, ms:T) {
  def num = lb{width{T}}
  def V = [2]T
  m := V**ms
  x := V**xs & m
  d := ~m << 1           # One bit of the position difference at x
  c := V**(1<<64-1)
  @unroll (i to num) {
    def sh = 1 << i
    def shift_at{v, s} = { v = (v&~s) | (v&s)>>sh }
    p := clmul{d, c, 0}  # xor-scan
    d = d &~ p           # Remove even bits
    p &= m
    shift_at{m, p}
    shift_at{x, p}
  }
  extract{x, 0}
}

fn pext{T & hasarch{'BMI2'}}(x:T, m:T) = pext{x, m}

export{'si_pext_u64', pext{u64}}
