include './base'
if (hasarch{'BMI2'}) {
  include './bmi2'
}
if (hasarch{'X86_64'}) {
  include './sse'
}
if (hasarch{'PCLMUL'}) {
  def clmul{a:T, b:T, imm & w128i{T}} = emit{T, '_mm_clmulepi64_si128', a, b, imm}
} else {
  def clmul{...x} = assert{'clmul not supported', show{...x}}
}
if (hasarch{'AVX2'}) {
  include './avx'
  include './avx2'
}
include './mask'
include 'util/tup'

# Modifies the input variable r
# Assumes iter{} will increment r, by at most write_len
def for_special_buffered{r, write_len}{vars,begin,sum,iter} = {
  assert{isreg{r}}; assert{begin==0}
  def T = eltype{type{r}}; def tw = width{T}
  def ov = write_len-1
  buf := undefined{T, 2*(ov+1)}
  r0 := r
  end := r + sum - ov
  i:u64 = 0; buf_used:u1 = 0
  def restart = setlabel{}
  while (r < end) {
    iter{i, vars}
    ++i
  }
  if (not buf_used) {
    end += buf - r + ov
    if (buf < end) {
      r0 = r
      r = buf
      buf_used = 1; goto{restart}
    }
  } else {
    def vc = 256/tw;
    if (hasarch{'AVX2'} and write_len >= vc) {
      def R = [vc]T
      if (ov>vc and end-buf>vc) { store{*R~~r0, 0, load{*R~~buf}}; r0+=vc; buf+=vc }
      homMaskStoreF{*R~~r0, maskOf{R, end-buf}, load{*R~~buf}}
    } else {
      @for (r0, buf over u64~~(end-buf)) r0 = buf
    }
  }
}

def storeu{p:T, i, v:eltype{T} & *u64==T} = emit{void, 'storeu_u64', p+i, v}
def loadu{p:T & *u64==T} = emit{eltype{T}, 'loadu_u64', p}

# Assumes w is trimmed, so the last 1 appears at index l-1
fn slash2{T}(w:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  def bitp_get{arr, n} = (load{arr,n>>6} >> (n&63)) & 1
  @for (x over i to l) {
    store{r, 0, x}
    r+= bitp_get{w,i}
  }
}

fn slash2{T==i8 & hasarch{'X86_64'}}(w:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  def U = [16]u8
  k1 := U**1
  @for_special_buffered{r,16} (w in *u16~~w, x0 in *U~~x over sum) {
    bm  := make{U, 1<<(iota{16}%8)}
    rb  := make{U, replicate{8,each{bind{cast_i,u8},tup{w,w>>8}}}}
    bit := rb&bm == bm  # Bits of w expanded to a byte each
    x := x0&bit
    dif := k1 + bit
    # Prefix sum halves of dif
    @unroll (k to 3) dif += U~~([2]i64~~dif << (8<<k))
    pc  := each{{j} => 8 - (extract{[8]u16~~dif, j} >> 8), tup{3,7}}
    dif = U~~([2]i64~~dif << 8)
    # Shift each value in x down by the corresponding one in dif
    b := k1
    @unroll (k to 3) {
      m := (dif & b) == b # Mask of positions to shift
      x = shr{U, x&m, 1<<k} | (x&~m)
      dif = max{dif, shr{U, dif&m, 1<<k}}
      b += b
    }
    each{
      {ins,c} => { emit{void, ins, *[8]u8~~r, x}; r+=c },
      tup{'_mm_storel_pi','_mm_storeh_pi'},
      pc
    }
  }
}

def comp8{w:*u64, X, r:*i8, l:u64, sum:u64} = {
  @for_special_buffered{r,8} (w in *u8~~w over sum) {
    pc:= popc{w}
    storeu{*u64~~r, 0, pext{promote{u64,X{}}, pdep{promote{u64, w}, cast{u64,0x0101010101010101}}*255}}
    r+= pc
  }
}

def tab{n,l} = {
  def m=n-1; def t=tab{m,l}
  def k = (1<<l - 1) << (m*l)
  merge{t, k+t}
}
def tab{n==0,l} = tup{0}
c16lut:*u64 = tab{4,16}

def vgLoad{p:T, i & T == *u64} = emit{eltype{T}, 'vg_loadLUT64', p, i}

def comp16{w:*u64, X, r:*i16, l:u64, sum:u64} = {
  @for_special_buffered{r,8} (w in *u8~~w over sum) {
    def step{r, w} = {
      storeu{*u64~~r, 0, pext{promote{u64,X{}}, vgLoad{c16lut, w}}}
    }
    rs:= r; r+= popc{w} # Measured slow incrementing at the end
    h := w&0xf
    step{rs, h}
    step{rs+popcRand{h}, w>>4}
  }
}

fn slash2{F, T}(w:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  xv:= *u64~~x
  F{w, {} => {c:= loadu{xv}; xv+= 1; c}, r, l, sum}
}

fn slash1{F, T, iota, add}(w:*u64, r:*T, l:u64, sum:u64) : void = {
  x:u64 = iota
  F{w, {} => {c:= x; x+= add; c}, r, l, sum}
}

def get_comp{T & width{T}==8 } = comp8
def get_comp{T & width{T}==16} = comp16
def slash1{T & hasarch{'BMI2'}} = {
  def w = width{T}
  def n = 64/w
  def b = bind{base, 1<<w}
  slash1{get_comp{T}, T, b{iota{n}}, b{n**n}}
}
def slash2{T & hasarch{'BMI2'}} = slash2{get_comp{T}, T}

fn slash2{T==i8 & hasarch{'AVX2'}}(w:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  def I = [32]i8
  def S = [8]u32
  def s8 = bind{sel,[16]u8}
  def mI{t} = make{I, merge{t,t}}
  io := mI{iota{16}}
  tr4x4 := mI{join{flip{split{4,iota{16}}}}}

  sumtab := mI{flat_table{{...a}=>fold{+,a}, ... 4**iota{2}} - 4}
  def ind4{b} = shiftright{indices{reverse{b}}-iota{fold{+,b}},4**0}
  def ind2x2{...b} = base{4, ind4{b}}
  itab := mI{flat_table{ind2x2, ... 4**iota{2}}}

  @for_special_buffered{r,32} (w in *u32~~w, x in *[32]T~~x over sum) {
    def step{k==1} = { # Unused, ~10% slower
      bit := I~~make{[32]u8, 1<<(iota{32}%8)}
      sum := I~~(s8{I~~S**w, make{I,iota{32}>>3}}&bit != bit)
      tup{sum + shl{[16]u8, sum, 1}, io - sum}
    }
    def step{k==2} = {
      wv := I~~(S**w >> make{S,4*iota{8}}) & I**0xf
      sum:= s8{sumtab, wv}
      ws := s8{itab, s8{wv, mI{4*(iota{16}%4)}}}
      w4 := io + s8{I~~(S~~ws >> make{S,2*(iota{8}%4)}) & I**3, tr4x4}
      tup{shl{[16]u8, sum, 3}, w4}
    }
    def step{k & k>2} = {
      def h = k-1
      {sum, res} := step{h}
      ik := mI{zlow{k,iota{16}} + (1<<h - 1)}
      mh := mI{-(iota{16}>>h & 1)}
      ss := s8{sum, ik}
      tup{sum+ss, max{res, s8{res & mh, io - ss}}}
    }
    {_,j16} := step{4}
    r16 := s8{x, j16}

    store{*[16]T~~r,                  0, half{r16, 0}}
    store{*[16]T~~(r+popc{w&0xffff}), 0, half{r16, 1}}
    r += popc{w}
  }
}

itab  :*u64 = fold{{t,k} => join{each{tup,t,k+(t<<8)%(1<<64)}}, tup{0x8080808080808080}, reverse{iota{8}}}
i64tab:*u32 = fold{{t,k} => join{each{tup,t,k+(t<<8)%(1<<32)}}, tup{0x80808080}, reverse{2*iota{4}}}

fn slash2{T & hasarch{'AVX2'} & width{T}>=32}(wp:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def V = [8]u32
  expander := make{[32]u8, merge{...each{{i}=>tup{i, ... 3**128}, iota{8}>>lb{tw/32}}}}
  def tab = if (tw==32) itab else i64tab
  def step{w,i} = {
    pc := popc{w}
    ind := load{tab, w}; def I = type{ind}
    s := sel{[16]i8, V~~[width{V}/width{I}]I**ind, expander}
    if (tw==64) s |= make{V, iota{8}%2}
    store{*V~~r, 0, sel{V, load{*V~~x,i}, s}}
    r+= pc
  }
  @for_special_buffered{r,8} (w in *u8~~wp over i to sum) {
    if (tw==32) {
      step{w, i}
    } else {
      step{w&0xf, 2*i}
      step{w>>4, 2*i+1}
    }
  }
}

fn slash2{T & hasarch{if (width{T}>=32) 'AVX512F' else 'AVX512VBMI2'}}(w:*u64, x:*T, r:*T, l:u64, sum:u64) : void = {
  def f = match { {_==8}=>'8'; {_==16}=>'16'; {_==32}=>'32'; {_==64}=>'64' }
  def wt = width{T}
  def vl = 512/wt
  def V = [vl]T
  def wu = max{32,vl}
  def load {a:T, n & 512==width{eltype{T}}} = emit{eltype{T}, '_mm512_loadu_si512', a+n}
  @for (w in *(ty_u{vl})~~w, x in *V~~x over cdiv{l,vl}) {
    def I = ty_u{wu}
    def emitT{O, name, ...a} = emit{O, merge{'_mm512_',name,'_epi',f{wt}}, ...a}
    def to_mask{a} = emit{[vl]u1, merge{'_cvtu',f{wu},'_mask',f{vl}}, a}
    m := to_mask{promote{I,w}}
    c := popc{w}
    # The compress-store instruction performs very poorly on Zen4,
    # and is also a lot worse than the following on Tiger Lake
    # emitT{void, 'mask_compressstoreu', r, m, x}
    cs := cast_i{I,promote{i64,1}<<c - 1}
    if (wu==64) cs -= cast_i{I,c}>>6
    v := emitT{V, 'mask_compress', x, m, x}
    emitT{void, 'mask_storeu', r, to_mask{cs}, v}
    r += c
  }
}

if (hasarch{'BMI2'}) {
export{'bmipopc_1slash8', slash1{i8}}
export{'bmipopc_1slash16', slash1{i16}}
}
export{'si_2slash8',  slash2{i8}}
export{'si_2slash16', slash2{i16}}
export{'si_2slash32', slash2{i32}}
export{'si_2slash64', slash2{i64}}

# pext, or boolean compress
fn pext{T}(x:T, m:T) {
  def w = width{T}
  def mod{a} = a % (1<<w)
  def lowbits{k} = base{1<<k, cdiv{w,k}**1}
  # At each step, x and z are split into groups of length k
  # - z tells how many bits in the group are NOT used
  # - x contains the bits, with z zeros above
  def build{k==1} = tup{x&m, ~m}
  def build{k & k > 1} = {
    def h = k>>1          # Increase size from h to k
    {x,z} := build{h}
    def low = lowbits{k}  # Low bit in each new group
    if (k <= 3) {
      z0 := z & low
      zm := z>>1 & low
      if (k == 2) tup{
        x - (x>>1 & z0),
        z0 + zm
      } else tup{ # Faster 1->3 jump, currently unused
        x - ((x>>1&mod{low*3}) & (z|z0<<1)) - (x>>2 & (z & zm)),
        (z0 + zm) + (z>>2 & low)
      }
    } else {
      # Shift high x group down by low z, then add halves of z
      even:T = mod{low*(1<<h - 1)}
      # SWAR shifter: shift x by sh*o, in length-k groups
      def shift{sh, o, x} = {
        l := o & low; m := l<<k - l
        s := (x & m)>>sh | (x &~ m)
        if (2*sh<=k/2) shift{2*sh, o>>1, s} else s
      }
      tup{
        (x&even) | shift{1, z, x&~even},
        if (k>4) (z + z>>h)&even else ((z&~even)>>h) + (z&even)
      }
    }
  }
  # Finally, compose groups with regular shifts
  def g = 8  # 12 performs about the same
  {b,z} := build{g}
  o := z*lowbits{g}  # Offsets by prefix sum
  def s = 1<<g - 1
  def gr{sh} = (b & mod{s<<sh}) >> (o>>(sh-g) & s)
  fold{|, b&s, each{gr, g*slice{iota{cdiv{w,g}},1}}}
}

fn pext{T & hasarch{'PCLMUL'} & T==u64}(xs:T, ms:T) {
  def num = lb{width{T}}
  def vec{s} = make{[2]T, s, 0}
  m := vec{ms}
  x := vec{xs} & m
  d := ~m << 1           # One bit of the position difference at x
  c := vec{1<<64-1}
  @unroll (i to num) {
    def sh = 1 << i
    def shift_at{v, s} = { v = (v&~s) | (v&s)>>sh }
    p := clmul{d, c, 0}  # xor-scan
    d = d &~ p           # Remove even bits
    p &= m
    shift_at{m, p}
    shift_at{x, p}
  }
  extract{x, 0}
}

fn pext{T & hasarch{'BMI2'}}(x:T, m:T) = pext{x, m}

export{'si_pext_u64', pext{u64}}
