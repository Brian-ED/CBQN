include './base'
if (hasarch{'X86_64'}) include './sse'
if (hasarch{'PCLMUL'}) include './clmul'
if (hasarch{'AVX2'}) { include './avx'; include './avx2' }
if (hasarch{'BMI2'}) include './bmi2'
if (hasarch{'AARCH64'}) include './neon'
if (hasarch{'AVX512F'}) {
  local def mti{s,T} = merge{'_mm512_',s,'_epi',fmtnat{elwidth{T}}}
  def load{a:T, n & 512==width{eltype{T}}} = emit{eltype{T}, '_mm512_loadu_si512', a+n}
  def make{T, xs & 512==width{T} & tuplen{xs}==vcount{T}} = {
    def p = each{{c}=>promote{eltype{T},c},reverse{xs}}
    emit{T, mti{'set',T}, ...p}
  }
  def iota{T & isvec{T} & 512==width{T}} = make{T, iota{vcount{T}}}
  def broadcast{T, v & isvec{T} & 512==width{T}} = {
    emit{T, mti{'set1',T}, promote{eltype{T},v}}
  }
  def __add{a:T,b:T & 512==width{T}} = emit{T, mti{'add',T}, a, b}
}
include './mask'
include 'util/tup'

def popcRand{x:T & isint{T} & width{T}==64} = emit{u8, 'rand_popc64', x} # under valgrind, return a random result in the range of possible ones
def popcRand{x:T & isint{T} & width{T}<=32} = emit{u8, 'rand_popc64', x}

# Table from l bits to w-bit indices, shifted left by s, and G applied afterwards
def maketab{l,w,s,G} = {
  def bot = fold{
    {t,k} => join{each{tup, t, G{k} + t<<w}},
    tup{0},
    reverse{iota{l}<<s}
  }
  # Store popcnt-1 in the high element
  def top = (fold{bind{flat_table,+}, l**iota{2}} - 1)%(1<<(w-s))
  top<<(l*w-w+s) | bot  # Overlaps for all-1 value only
}
def maketab{l,w,s} = maketab{l,w,s,{x}=>x}
def maketab{l,w} = maketab{l,w,0}

itab:*u64 = maketab{8,8} # 256 elts, 2KB; shared by many methods

# Recover popcount, for when POPCNT isn't there
def has_popc = hasarch{'POPCNT'}
def tab_popc{i:I, w} = (i>>(width{I}-w) + 1) & (1<<w - 1)
def popc_alt{v, i, w} = if (has_popc) popc{v} else tab_popc{i, w}

# slash{c, T} defines:
#   if c==1: w/x
#   if c==0 & (T==i8 or T==i16): /w
#   if c==0 & T==i32: x + /w (assumes x is a multiple of 8 for topper)
# if sum(w) < len/thresh{c,T}, sparse Where will be used
def arg{c,T} = if (c) *T else if (T==i32) T else tup{} # type of x

# Modifies the input variable r
# Assumes iter{} will increment r, by at most write_len
def for_special_buffered{r, write_len}{vars,begin,sum,iter} = {
  assert{isreg{r}}; assert{begin==0}
  def T = eltype{type{r}}; def tw = width{T}
  def ov = write_len-1
  buf := undefined{T, 2*(ov+1)}
  r0 := r
  end := r + sum - ov
  i:u64 = 0; buf_used:u1 = 0
  def restart = setlabel{}
  while (r < end) {
    iter{i, vars}
    ++i
  }
  if (not buf_used) {
    end += buf - r + ov
    if (buf < end) {
      r0 = r
      r = buf
      buf_used = 1; goto{restart}
    }
  } else {
    if (has_simd) {
      def vc = arch_defvw/tw;
      def R = [vc]T
      @unroll ((ov/vc)>>0) if (end-buf>vc) { store{*R~~r0, 0, load{*R~~buf}}; r0+=vc; buf+=vc }
      homMaskStoreF{*R~~r0, maskOf{R, end-buf}, load{*R~~buf}}
    } else {
      @for (r0, buf over u64~~(end-buf)) r0 = buf
    }
  }
}

# Assumes w is trimmed, so the last 1 appears at index l-1
# Unused because an index buffer and select is faster
def thresh{c, T} = 1
fn slash{c, T}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def bitp_get{arr, n} = (load{arr,n>>6} >> (n&63)) & 1
  @for (i to l) {
    store{r, 0, if (c) load{x,i} else if (T==i32) cast_i{T,i}+x else i}
    r+= bitp_get{w,i}
  }
}

def getter{c, V, x} = {
  if (c) {
    i:u64 = 0
    {} => { v:=load{*V~~x, i}; ++i; v }
  } else {
    i := iota{V}
    if (isreg{x}) i += V**cast_i{eltype{V},x}
    ii := V**vcount{V}
    {} => { v:=i; i+=ii; v }
  }
}

# Top bits to convert 1-byte indices to 2 or 4
# These can only change between loop iterations, provided the
# given x for i32 is a multiple of the loop step
def topper{T, U, k, x} = {
  def make_top{S} = re_el{S,U}**(if (T<i32) 0 else cast_i{S, x>>width{S}})
  top := each{make_top, replicate{{S}=>S<T, tup{i8,i16}}}
  def i_off = if (T<i32) 0 else { assert{x%k==0}; cast_i{u64, x/k} }
  # Increment top vector when i*k passes width of bottom vector
  def vb = lb{k}
  def inc{i, {}} = {}
  def inc{i, {t:V, ...ts}} = {
    if ((i+1+i_off)%(1<<(elwidth{V}-vb)) == 0) { t += V**1; inc{i,ts} }
  }
  tup{top, inc}
}

# i8 & i16 /w; 64 bits/iter; SWAR
itab_4_16:*u64 = maketab{4,16} # 16 elts, 128B
def thresh{c==0, T==i8 } = 32
def thresh{c==0, T==i16} = 16
fn slash{c==0, T & T<=i16}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def n = 64/tw
  def tab = if (tw==8) itab else itab_4_16
  j:u64 = 0
  def inc = base{1<<tw, n**n}
  @for_special_buffered{r,8} (w in *u8~~w over sum) {
    def rn = if (has_popc) r+popc{w} else 0
    def step{w} = {
      i := load{tab, w}
      storeu{*u64~~r, j + i}
      r += popc_alt{w, i, tw}
      j += inc
    }
    if (tw==8) { step{w} }
    else { step{w&0xf}; step{w>>4} }
    if (has_popc) r = rn  # Shorter dependency chain
  }
}

# i16 /w & i32 x+/w; 8 elts/iter; 64 bit table input, expanded to 128 or 256 via topper
def simd128{} = hasarch{'X86_64'} | hasarch{'AARCH64'}
def thresh{c==0, T==i16 & simd128{}} = 32
def thresh{c==0, T==i32 & simd128{}} = 16
fn slash{c==0, T & simd128{} & i16<=T & T<=i32}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def I = [16]i8
  j := I**(if (T==i16) 0 else cast_i{i8,x})
  def {top, inctop} = topper{T, I, 8, x}
  @for_special_buffered{r,8} (w in *u8~~w over i to sum) {
    ind := load{itab, w}
    pc := popc_alt{w, ind, 8}
    v := mzipLo{j + I~~make{[2]u64, ind, 0}, tupsel{0,top}}
    def st{k, v:V} = store{*V~~r, k, v}
    if (T==i16) st{0, v}
    else each{st, iota{2}, mzip{v, tupsel{1,top}}}
    r += pc
    j += I**8
    inctop{i, top}
  }
}

# i8 & i16 w/x; 128 bits/iter; [16]i8 shuffle
def shufb128{} = hasarch{'SSSE3'} | hasarch{'AARCH64'}
def thresh{c==1, T==i8  & shufb128{}} = 64
def thresh{c==1, T==i16 & shufb128{}} = 32
fn slash{c==1, T & T<=i16 & shufb128{}}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def V = [16]i8
  @for_special_buffered{r,8} (w in *u8~~wp over i to sum) {
    ind := load{itab, w}
    pc := popc_alt{w, ind, 8}
    s := V~~make{[2]u64, ind,0}
    if (T==i16) { s+=s; s = V~~mzipLo{s, s+V**1} }
    res := sel{V, load{*V~~(x+8*i)}, s}
    if (T==i8) store{*u64~~r, 0, extract{[2]u64~~res, 0}}
    else store{*V~~r, 0, res}
    r+= pc
  }
}

# i32 w/x; 8 elts/iter into 2 steps; [16]i8 shuffle
i32tab:*u32 = maketab{4,8,2} # 16 elts, 64B
def thresh{c==1, T==i32 & shufb128{}} = 8
fn slash{c==1, T==i32 & shufb128{}}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def V = [16]i8
  expander := make{V, iota{16}>>2}
  trail := make{V, tail{2,iota{16}}}
  def step{w,i} = {
    ind := load{i32tab, w}
    pc := popc_alt{w, ind, 6}
    s := sel{[16]i8, V~~make{[4]u32, ind, ... 3**0}, expander} | trail
    res := sel{V, load{*V~~(x+4*i)}, s}
    store{*V~~r, 0, res}
    r+= pc
  }
  @for_special_buffered{r,8} (w in *u8~~wp over i to sum) {
    def rn = if (has_popc) r+popc{w} else 0
    step{w&0xf, 2*i}
    step{w>>4, 2*i+1}
    if (has_popc) r = rn
  }
}

# i32 & i64 w/x & x+/w; 256 bits/step, 8 elts/iter; [8]i32 shuffle
i64tab:*u64 = maketab{4,16,1,{x}=>(1+x)*0x100 + x} # 16 elts, 128B
def thresh{c, T==i32 & hasarch{'AVX2'}} = 32
def thresh{c, T==i64 & hasarch{'AVX2'}} = 8
fn slash{c, T & hasarch{'AVX2'} & T>=i32}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def V = [8]u32
  expander := make{[32]u8, merge{...each{{i}=>tup{i, ... 3**128}, iota{8}>>lb{tw/32}}}}
  def from_ind = if (c) {
    i:u64 = 0
    {j} => { v:=load{*V~~x, i}; ++i; sel{V, v, j} }
  } else if (T==i32) {
    def VT = [8]T
    i := VT**x
    ii := VT**8
    {j} => { v:=i+VT~~j; i+=ii; v }
  }
  def tab = if (tw==32) itab else i64tab
  def step{r, w} = {
    s:= loadBatch{*u8~~(tab+w), 0, V}
    store{*V~~r, 0, from_ind{s}}
  }
  @for_special_buffered{r,8} (w in *u8~~wp to sum) {
    pc := popc{w}
    if (tw==32) {
      step{r,w}
    } else {
      h := w&0xf
      step{r, h}
      step{r+popcRand{h}, w>>4}
    }
    r += pc
  }
}

# everything; 512 bits/iter; AVX-512 compress
def thresh{c, T==i8  & hasarch{'AVX512VBMI2'}} = 256
def thresh{c, T==i16 & hasarch{'AVX512VBMI2'}} = 128
def thresh{c, T==i32 & hasarch{'AVX512F'}}     = 64
def thresh{c, T==i64 & hasarch{'AVX512F'}}     = 16
fn slash{c, T & hasarch{if (width{T}>=32) 'AVX512F' else 'AVX512VBMI2'}}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def f = fmtnat
  def wt = width{T}
  def vl = 512/wt
  def V = [vl]T
  def X = getter{c, V, x}
  def wu = max{32,vl}
  @for (w in *(ty_u{vl})~~w over cdiv{l,vl}) {
    def I = ty_u{wu}
    def emitT{O, name, ...a} = emit{O, merge{'_mm512_',name,'_epi',f{wt}}, ...a}
    def to_mask{a} = emit{[vl]u1, merge{'_cvtu',f{wu},'_mask',f{vl}}, a}
    m := to_mask{promote{I,w}}
    c := popc{w}
    x := X{}
    # The compress-store instruction performs very poorly on Zen4,
    # and is also a lot worse than the following on Tiger Lake
    # emitT{void, 'mask_compressstoreu', r, m, x}
    cs := cast_i{I,promote{i64,1}<<(c%64) - 1}
    if (wu==64) cs -= cast_i{I,c}>>6
    v := emitT{V, 'mask_compress', x, m, x}
    emitT{void, 'mask_storeu', r, to_mask{cs}, v}
    r += c
  }
}

export{'si_1slash8' , slash{0, i8 }}
export{'si_1slash16', slash{0, i16}}; export{'si_thresh_1slash16', u64~~thresh{0, i16}}
export{'si_1slash32', slash{0, i32}}; export{'si_thresh_1slash32', u64~~thresh{0, i32}}
export{'si_2slash8' , slash{1, i8 }}; export{'si_thresh_2slash8' , u64~~thresh{1, i8 }}
export{'si_2slash16', slash{1, i16}}; export{'si_thresh_2slash16', u64~~thresh{1, i16}}
export{'si_2slash32', slash{1, i32}}; export{'si_thresh_2slash32', u64~~thresh{1, i32}}
export{'si_2slash64', slash{1, i64}}; export{'si_thresh_2slash64', u64~~thresh{1, i64}}

# pext, or boolean compress
fn pext{T}(x:T, m:T) {
  def w = width{T}
  def mod{a} = a % (1<<w)
  def lowbits{k} = base{1<<k, cdiv{w,k}**1}
  # At each step, x and z are split into groups of length k
  # - z tells how many bits in the group are NOT used
  # - x contains the bits, with z zeros above
  def build{k==1} = tup{x&m, ~m}
  def build{k & k > 1} = {
    def h = k>>1          # Increase size from h to k
    {x,z} := build{h}
    def low = lowbits{k}  # Low bit in each new group
    if (k <= 3) {
      z0 := z & low
      zm := z>>1 & low
      if (k == 2) tup{
        x - (x>>1 & z0),
        z0 + zm
      } else tup{ # Faster 1->3 jump, currently unused
        x - ((x>>1&mod{low*3}) & (z|z0<<1)) - (x>>2 & (z & zm)),
        (z0 + zm) + (z>>2 & low)
      }
    } else {
      # Shift high x group down by low z, then add halves of z
      even:T = mod{low*(1<<h - 1)}
      # SWAR shifter: shift x by sh*o, in length-k groups
      def shift{sh, o, x} = {
        l := o & low; m := l<<k - l
        s := (x & m)>>sh | (x &~ m)
        if (2*sh<=k/2) shift{2*sh, o>>1, s} else s
      }
      tup{
        (x&even) | shift{1, z, x&~even},
        if (k>4) (z + z>>h)&even else ((z&~even)>>h) + (z&even)
      }
    }
  }
  # Finally, compose groups with regular shifts
  def g = 8  # 12 performs about the same
  {b,z} := build{g}
  o := z*lowbits{g}  # Offsets by prefix sum
  def s = 1<<g - 1
  def gr{sh} = (b & mod{s<<sh}) >> (o>>(sh-g) & s)
  fold{|, b&s, each{gr, g*slice{iota{cdiv{w,g}},1}}}
}

fn pext{T & hasarch{'PCLMUL'} & T==u64}(xs:T, ms:T) {
  def num = lb{width{T}}
  def vec{s} = make{[2]T, s, 0}
  m := vec{ms}
  x := vec{xs} & m
  d := ~m << 1           # One bit of the position difference at x
  c := vec{1<<64-1}
  @unroll (i to num) {
    def sh = 1 << i
    def shift_at{v, s} = { v = (v&~s) | (v&s)>>sh }
    p := clmul{d, c, 0}  # xor-scan
    d = d &~ p           # Remove even bits
    p &= m
    shift_at{m, p}
    shift_at{x, p}
  }
  extract{x, 0}
}

fn pext{T & hasarch{'BMI2'}}(x:T, m:T) = pext{x, m}

export{'si_pext_u64', pext{u64}}
