include './base'
if (hasarch{'BMI2'}) {
  include './bmi2'
}
if (hasarch{'X86_64'}) {
  include './sse'
}
if (hasarch{'PCLMUL'}) {
  def clmul{a:T, b:T, imm & w128i{T}} = emit{T, '_mm_clmulepi64_si128', a, b, imm}
} else {
  def clmul{...x} = assert{'clmul not supported', show{...x}}
}
if (hasarch{'AVX2'}) {
  include './avx'
  include './avx2'
}
if (hasarch{'AVX512F'}) {
  local def mti{s,T} = merge{'_mm512_',s,'_epi',fmtnat{elwidth{T}}}
  def load{a:T, n & 512==width{eltype{T}}} = emit{eltype{T}, '_mm512_loadu_si512', a+n}
  def make{T, xs & 512==width{T} & tuplen{xs}==vcount{T}} = {
    def p = each{{c}=>promote{eltype{T},c},reverse{xs}}
    emit{T, mti{'set',T}, ...p}
  }
  def broadcast{T, v & isvec{T} & 512==width{T}} = {
    emit{T, mti{'set1',T}, promote{eltype{T},v}}
  }
  def __add{a:T,b:T & 512==width{T}} = emit{T, mti{'add',T}, a, b}
}
include './mask'
include 'util/tup'

def arg{c,T} = if (c) *T else if (T==i32) T else tup{}

# Modifies the input variable r
# Assumes iter{} will increment r, by at most write_len
def for_special_buffered{r, write_len}{vars,begin,sum,iter} = {
  assert{isreg{r}}; assert{begin==0}
  def T = eltype{type{r}}; def tw = width{T}
  def ov = write_len-1
  buf := undefined{T, 2*(ov+1)}
  r0 := r
  end := r + sum - ov
  i:u64 = 0; buf_used:u1 = 0
  def restart = setlabel{}
  while (r < end) {
    iter{i, vars}
    ++i
  }
  if (not buf_used) {
    end += buf - r + ov
    if (buf < end) {
      r0 = r
      r = buf
      buf_used = 1; goto{restart}
    }
  } else {
    def vc = 256/tw;
    if (hasarch{'AVX2'} and write_len >= vc) {
      def R = [vc]T
      if (ov>vc and end-buf>vc) { store{*R~~r0, 0, load{*R~~buf}}; r0+=vc; buf+=vc }
      homMaskStoreF{*R~~r0, maskOf{R, end-buf}, load{*R~~buf}}
    } else {
      @for (r0, buf over u64~~(end-buf)) r0 = buf
    }
  }
}

def storeu{p:T, i, v:eltype{T} & *u64==T} = emit{void, 'storeu_u64', p+i, v}
def loadu{p:T & *u64==T} = emit{eltype{T}, 'loadu_u64', p}

# Assumes w is trimmed, so the last 1 appears at index l-1
def thresh{c, T} = 2
fn slash{c, T}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def bitp_get{arr, n} = (load{arr,n>>6} >> (n&63)) & 1
  @for (i to l) {
    store{r, 0, if (c) load{x,i} else if (T==i32) cast_i{T,i}+x else i}
    r+= bitp_get{w,i}
  }
}

def getter{c, V, x} = {
  if (c) {
    i:u64 = 0
    {} => { v:=load{*V~~x, i}; ++i; v }
  } else {
    def k = vcount{V}
    i := make{V, iota{k}}
    if (isreg{x}) i += V**cast_i{eltype{V},x}
    ii := V**k
    {} => { v:=i; i+=ii; v }
  }
}

def thresh{c, T & hasarch{'X86_64'} & T<=(if (c) i8 else i32)} = 4
fn slash{c, T & hasarch{'X86_64'} & T<=(if (c) i8 else i32)}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def U = [16]u8
  k1 := U**1
  def X = getter{c, U, x}
  def make_top{S} = to_el{S,U}**(if (T<i32) 0 else cast_i{S, x>>width{S}})
  top := each{make_top, replicate{{S}=>S<T, tup{i8,i16}}}
  def i_off = if (T<i32) 0 else { assert{x%16==0}; cast_i{u64, x/16} }
  @for_special_buffered{r,16} (w in *u16~~w over i to sum) {
    x := X{}
    bm  := make{U, 1<<(iota{16}%8)}
    rb  := make{U, replicate{8,each{bind{cast_i,u8},tup{w,w>>8}}}}
    bit := rb&bm == bm  # Bits of w expanded to a byte each
    x &= bit
    dif := k1 + bit
    # Prefix sum halves of dif
    @unroll (k to 3) dif += U~~([2]i64~~dif << (8<<k))
    pc  := each{{j} => 8 - (extract{[8]u16~~dif, j} >> 8), tup{3,7}}
    dif = U~~([2]i64~~dif << 8)
    # Shift each value in x down by the corresponding one in dif
    b := k1
    @unroll (k to 3) {
      m := (dif & b) == b # Mask of positions to shift
      y := shr{U, x&m, 1<<k}
      x = (if (c) (x&~m)|y else max{x, y})
      dif = max{dif, shr{U, dif&m, 1<<k}}
      b += b
    }
    def each_pc{gen, ...par} = each{{...p,c} => { gen{...p}; r+=c }, ...par, pc}
    if (T==i8) { # 0==tuplen{top}
      def st{ins} = emit{void, ins, *[8]u8~~r, x}
      each_pc{st, tup{'_mm_storel_pi','_mm_storeh_pi'}}
    } else {
      def st{k, v:V} = store{*V~~r, k, v}
      def st{v} = if (T==i16) st{0, v}
                  else each{st, iota{2}, unpack{v, tupsel{1,top}}}
      each_pc{st, unpack{[16]i8~~x, tupsel{0,top}}}
    }
    # Increment top vector when i*16 passes width of bottom vector
    def inc{{}} = {}
    def inc{{t:V, ...ts}} = {
      if ((i+1+i_off)%(1<<(elwidth{V}-4)) == 0) { t += V**1; inc{ts} }
    }
    inc{top}
  }
}

def tab{n,l} = if (n==0) tup{0} else {
  def m = n-1
  def k = (1<<l - 1) << (m*l)
  flat_table{+, tup{0,k}, tab{m,l}}
}
c16lut:*u64 = tab{4,16}
def vgLoad{p:T, i & T == *u64} = emit{eltype{T}, 'vg_loadLUT64', p, i}

def thresh{c, T==i8  & hasarch{'BMI2'}} = 32
def thresh{c, T==i16 & hasarch{'BMI2'}} = 16
fn slash{c, T & hasarch{'BMI2'}}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def wt = width{T}
  def b = bind{base, 1<<wt}
  def X = if (c) {
    xv:= *u64~~x
    {} => {c:= loadu{xv}; xv+= 1; c}
  } else {
    def n = 64/wt
    x:u64 = b{iota{n}}
    def add = b{n**n}
    {} => {c:= x; x+= add; c}
  }
  @for_special_buffered{r,8} (w in *u8~~w over sum) {
    pc:= popc{w}
    def out{r,e} = storeu{*u64~~r, 0, pext{promote{u64,X{}}, e}}
    if (wt == 8) {
      out{r, pdep{promote{u64, w}, u64~~b{8**1}}*255}
    } else {
      def step{r, w} = out{r, vgLoad{c16lut, w}}
      h := w&0xf
      step{r, h}
      step{r+popcRand{h}, w>>4}
    }
    r+= pc
  }
}

def thresh{c, T==i8  & hasarch{'AVX2'}} = 32
fn slash{c, T==i8 & hasarch{'AVX2'}}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def I = [32]i8
  def S = [8]u32
  def s8 = bind{sel,[16]u8}
  def mI{t} = make{I, merge{t,t}}
  io := mI{iota{16}}
  tr4x4 := mI{join{flip{split{4,iota{16}}}}}

  sumtab := mI{flat_table{{...a}=>fold{+,a}, ... 4**iota{2}} - 4}
  def ind4{b} = shiftright{indices{reverse{b}}-iota{fold{+,b}},4**0}
  def ind2x2{...b} = base{4, ind4{b}}
  itab := mI{flat_table{ind2x2, ... 4**iota{2}}}

  def from_ind = if (c) {
    i:u64 = 0
    {j} => { v:=load{*[32]T~~x, i}; ++i; s8{v, j} }
  } else {
    i := make{I, replicate{16,tup{0,16}}}
    ii := I**32
    {j} => { v:=i+j; i+=ii; v }
  }
  @for_special_buffered{r,32} (w in *u32~~w over sum) {
    def step{k==1} = { # Unused, ~10% slower
      bit := I~~make{[32]u8, 1<<(iota{32}%8)}
      sum := I~~(s8{I~~S**w, make{I,iota{32}>>3}}&bit != bit)
      tup{sum + shl{[16]u8, sum, 1}, io - sum}
    }
    def step{k==2} = {
      wv := I~~(S**w >> make{S,4*iota{8}}) & I**0xf
      sum:= s8{sumtab, wv}
      ws := s8{itab, s8{wv, mI{4*(iota{16}%4)}}}
      w4 := io + s8{I~~(S~~ws >> make{S,2*(iota{8}%4)}) & I**3, tr4x4}
      tup{shl{[16]u8, sum, 3}, w4}
    }
    def step{k & k>2} = {
      def h = k-1
      {sum, res} := step{h}
      ik := mI{zlow{k,iota{16}} + (1<<h - 1)}
      mh := mI{-(iota{16}>>h & 1)}
      ss := s8{sum, ik}
      tup{sum+ss, max{res, s8{res & mh, io - ss}}}
    }
    {_,j16} := step{4}
    r16 := from_ind{j16}

    store{*[16]T~~r,                  0, half{r16, 0}}
    store{*[16]T~~(r+popc{w&0xffff}), 0, half{r16, 1}}
    r += popc{w}
  }
}

itab  :*u64 = fold{{t,k} => join{each{tup,t,k+(t<<8)%(1<<64)}}, tup{0x8080808080808080}, reverse{iota{8}}}
i64tab:*u32 = fold{{t,k} => join{each{tup,t,k+(t<<8)%(1<<32)}}, tup{0x80808080}, reverse{2*iota{4}}}

def thresh{c, T==i32 & hasarch{'AVX2'}} = 32
def thresh{c, T==i64 & hasarch{'AVX2'}} = 8
fn slash{c, T & hasarch{'AVX2'} & width{T}>=32}(wp:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def tw = width{T}
  def V = [8]u32
  expander := make{[32]u8, merge{...each{{i}=>tup{i, ... 3**128}, iota{8}>>lb{tw/32}}}}
  def from_ind = if (c) {
    i:u64 = 0
    {j} => { v:=load{*V~~x, i}; ++i; sel{V, v, j} }
  } else if (T==i32) {
    def VT = [8]T
    i := VT**x
    ii := VT**8
    {j} => { v:=i+VT~~j; i+=ii; v }
  }
  def tab = if (tw==32) itab else i64tab
  def step{w} = {
    pc := popc{w}
    ind := load{tab, w}; def I = type{ind}
    s := sel{[16]i8, V~~[width{V}/width{I}]I**ind, expander}
    if (tw==64) s |= make{V, iota{8}%2}
    store{*V~~r, 0, from_ind{s}}
    r+= pc
  }
  @for_special_buffered{r,8} (w in *u8~~wp to sum) {
    if (tw==32) {
      step{w}
    } else {
      step{w&0xf}
      step{w>>4}
    }
  }
}

def thresh{c, T==i8  & hasarch{'AVX512VBMI2'}} = 256
def thresh{c, T==i16 & hasarch{'AVX512VBMI2'}} = 128
def thresh{c, T==i32 & hasarch{'AVX512F'}}     = 64
def thresh{c, T==i64 & hasarch{'AVX512F'}}     = 16
fn slash{c, T & hasarch{if (width{T}>=32) 'AVX512F' else 'AVX512VBMI2'}}(w:*u64, x:arg{c,T}, r:*T, l:u64, sum:u64) : void = {
  def f = fmtnat
  def wt = width{T}
  def vl = 512/wt
  def V = [vl]T
  def X = getter{c, V, x}
  def wu = max{32,vl}
  @for (w in *(ty_u{vl})~~w over cdiv{l,vl}) {
    def I = ty_u{wu}
    def emitT{O, name, ...a} = emit{O, merge{'_mm512_',name,'_epi',f{wt}}, ...a}
    def to_mask{a} = emit{[vl]u1, merge{'_cvtu',f{wu},'_mask',f{vl}}, a}
    m := to_mask{promote{I,w}}
    c := popc{w}
    x := X{}
    # The compress-store instruction performs very poorly on Zen4,
    # and is also a lot worse than the following on Tiger Lake
    # emitT{void, 'mask_compressstoreu', r, m, x}
    cs := cast_i{I,promote{i64,1}<<(c%64) - 1}
    if (wu==64) cs -= cast_i{I,c}>>6
    v := emitT{V, 'mask_compress', x, m, x}
    emitT{void, 'mask_storeu', r, to_mask{cs}, v}
    r += c
  }
}

export{'si_1slash8' , slash{0, i8 }}
export{'si_1slash16', slash{0, i16}}; export{'si_thresh_1slash16', u64~~thresh{0, i16}}
export{'si_1slash32', slash{0, i32}}; export{'si_thresh_1slash32', u64~~thresh{0, i32}}
export{'si_2slash8' , slash{1, i8 }}; export{'si_thresh_2slash8' , u64~~thresh{1, i8 }}
export{'si_2slash16', slash{1, i16}}; export{'si_thresh_2slash16', u64~~thresh{1, i16}}
export{'si_2slash32', slash{1, i32}}; export{'si_thresh_2slash32', u64~~thresh{1, i32}}
export{'si_2slash64', slash{1, i64}}; export{'si_thresh_2slash64', u64~~thresh{1, i64}}

# pext, or boolean compress
fn pext{T}(x:T, m:T) {
  def w = width{T}
  def mod{a} = a % (1<<w)
  def lowbits{k} = base{1<<k, cdiv{w,k}**1}
  # At each step, x and z are split into groups of length k
  # - z tells how many bits in the group are NOT used
  # - x contains the bits, with z zeros above
  def build{k==1} = tup{x&m, ~m}
  def build{k & k > 1} = {
    def h = k>>1          # Increase size from h to k
    {x,z} := build{h}
    def low = lowbits{k}  # Low bit in each new group
    if (k <= 3) {
      z0 := z & low
      zm := z>>1 & low
      if (k == 2) tup{
        x - (x>>1 & z0),
        z0 + zm
      } else tup{ # Faster 1->3 jump, currently unused
        x - ((x>>1&mod{low*3}) & (z|z0<<1)) - (x>>2 & (z & zm)),
        (z0 + zm) + (z>>2 & low)
      }
    } else {
      # Shift high x group down by low z, then add halves of z
      even:T = mod{low*(1<<h - 1)}
      # SWAR shifter: shift x by sh*o, in length-k groups
      def shift{sh, o, x} = {
        l := o & low; m := l<<k - l
        s := (x & m)>>sh | (x &~ m)
        if (2*sh<=k/2) shift{2*sh, o>>1, s} else s
      }
      tup{
        (x&even) | shift{1, z, x&~even},
        if (k>4) (z + z>>h)&even else ((z&~even)>>h) + (z&even)
      }
    }
  }
  # Finally, compose groups with regular shifts
  def g = 8  # 12 performs about the same
  {b,z} := build{g}
  o := z*lowbits{g}  # Offsets by prefix sum
  def s = 1<<g - 1
  def gr{sh} = (b & mod{s<<sh}) >> (o>>(sh-g) & s)
  fold{|, b&s, each{gr, g*slice{iota{cdiv{w,g}},1}}}
}

fn pext{T & hasarch{'PCLMUL'} & T==u64}(xs:T, ms:T) {
  def num = lb{width{T}}
  def vec{s} = make{[2]T, s, 0}
  m := vec{ms}
  x := vec{xs} & m
  d := ~m << 1           # One bit of the position difference at x
  c := vec{1<<64-1}
  @unroll (i to num) {
    def sh = 1 << i
    def shift_at{v, s} = { v = (v&~s) | (v&s)>>sh }
    p := clmul{d, c, 0}  # xor-scan
    d = d &~ p           # Remove even bits
    p &= m
    shift_at{m, p}
    shift_at{x, p}
  }
  extract{x, 0}
}

fn pext{T & hasarch{'BMI2'}}(x:T, m:T) = pext{x, m}

export{'si_pext_u64', pext{u64}}
