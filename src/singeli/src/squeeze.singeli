include './base'
include './mask'
include './cbqnDefs'
include 'util/tup'
include './vecfold'

def preserve_negative_zero = 0

# SSE2 versions avoid any 64-bit integer comparsions
def anySNaN{M, x:[_](u64)} = {
  homAny{inRangeLen{M{x}<<1, (0xFFE<<52)+2, (1<<52)-2}}
}
def anySNaN{M, x:T==[2]u64 if hasarch{'X86_64'} and not hasarch{'SSE4.2'}} = {
  topAny{M{andnot{unord{[2]f64~~x, [2]f64~~x}, [2]u64~~([4]u32**0xFFF8_0000 == ([4]u32~~x | [4]u32**0x8000_0000))}}}
}
def anyNonChar{M, x:[_](u64)} = homAny{M{~inRangeLen{x, cbqn_c32Tag{}<<48, 1<<48}}}
def anyNonChar{M, x:T=[_]_ if hasarch{'X86_64'}} = {
  def H = re_el{u32, T}
  def ne = H~~x  !=  H**cast_i{u32, cbqn_c32Tag{}<<16}
  topAny{M{T~~ne}}
}


def cvtNarrow{DE, x:[_]XE if width{DE}==width{XE}} = cvt{DE, x}
def cvtNarrow{DE, x:[_]XE if width{DE}< width{XE}} = narrow{DE, x}
def cvtWiden{  [_]DE, x:[_]XE if width{DE}==width{XE}} = cvt{DE, x}
def cvtWiden{D=[_]DE, x:[_]XE if width{DE}> width{XE}} = widen{D, x}

fn squeeze{vw, X, CHR, B}(x0:*void, len:ux) : u32 = {
  assert{len>0}
  def bulk = vw / width{X}
  def XV = [bulk]X
  def E = tern{X==f64, u32, ty_u{X}}
  def EV2 = [bulk*2]E
  def EV = tern{(width{E}*bulk == 64) & hasarch{'X86_64'}, EV2, [bulk]E}
  
  # fold with either Max or Bitwise Or, truncating/zero-extending to TE
  def foldTotal{TE, x:[_]T} = cast_i{TE, vfold{|, x}}
  def foldTotal{TE, x:[_]T if hasarch{'AARCH64'}} = {
    if (width{T}!=64) vfold{max, x}
    else if (width{TE}==64 and bulk==2) cast_i{TE, half{x,0} | half{x,1}}
    else vfold{max, narrow{TE, x}}
  }
  
  # show{XV, EV, CHR, B}
  xp:= *X~~x0
  r1:= EV**0
  if (CHR) { # c8, c16, c32
    def hw = width{E}/2
    @maskedLoop{bulk}(xv in tup{XV,xp}, M in 'm' over len) {
      c:= EV~~xv
      if (X!=u16) r1|= M{c} # for u64, just accept the garbage top 32 bits and deal with them at the end
      if (B) {
        if (anyNonChar{M, c}) return{3}
      } else {
        if (anynePositive{EV**((1<<hw-1)<<hw) & c, EV**0, M}) return{lb{hw}-2}
      }
    }
    r2:= foldTotal{u32, r1}
    if (X>u32 and r2>=65536) return{2}
    if (X>u16 and r2>=256) return{1}
    0
  } else { # i8, i16, i32, f64
    if (X==i8) { # i8
      @maskedLoop{bulk}(v0 in tup{XV,xp}, M in 'm' over len) {
        if (anynePositive{EV**0xfe & EV~~v0, EV**0, M}) return{2}
      }
      0
    } else { # i16, i32, f64
      def case_B = makeOptBranch{B, tup{ux}, {iCont} => {
        def XU = [bulk]u64
        @maskedLoop{bulk, iCont}(xv in tup{XV,xp}, M in 'm' over len) {
          v:= XU ~~ xv
          if (anySNaN{M, v}) return{0xffff_fffe} # not even a number
        }
        return{0xffff_ffff} # float
      }}
      
      def getAcc{EV=[_]E, x:[_]T} = {
        ((EV ** ~E~~1) & EV~~x)  ^  EV~~(x >> (width{T}-1))
      }
      
      if (isint{X}) { # i16, i32
        @muLoop{bulk, 1}(v0 in tup{XV,xp}, M in 'm' over len) {
          r1|= M{tree_fold{|, each{{v} => getAcc{EV, v}, v0}}}
        }
      } else { # f64
        r2:= EV2**0
        @muLoop{
          bulk, hasarch{'AARCH64'}+1,
          {} => { r1 = half{r2,0}|half{r2,1} }
        }(v0 in tup{XV,xp}, M in 'm' over is to len) {
          def int = {
            def {int, wdn} = {
              if (hasarch{'AARCH64'} and length{is}==2) {
                def intp = narrowPair{...each{cvt{i64,.}, v0}}
                def wdn = each{cvt{f64,.}, widen{intp}}
                tup{intp, wdn}
              } else {
                def ints = each{{v} => cvtNarrow{ty_s{E}, v}, v0}
                def wdn = each{{v} => cvtWiden{XV, v}, ints}
                def intp = match (...ints) {
                  {i:[(bulk)]_} => i
                  {i if hasarch{'X86_64'} and not hasarch{'AVX2'}} => i
                  {i:T=[(bulk)]_, j:T} => pair{ints}
                }
                tup{intp, wdn}
              }
            }
            
            def conv{x} = tern{preserve_negative_zero, ty_u{x}, x}
            def as = each{conv, v0}
            def bs = each{conv, wdn}
            def cond = {
              if (length{is}==1) anynePositive{...as, ...bs, M}
              else ~homAll{tree_fold{&, each{==, as, bs}}}
            }
            if (cond) { # is any not an integer
              if (B) case_B{select{is, 0}} # if B, need to give an even more special result
              else return{0xffff_ffff} # else, not integer => float
            }
            int
          }
          def acc = if (length{is}==2) r2 else r1
          
          acc|= M{getAcc{type{acc}, int}}
        }
      }
      
      
      def f = foldTotal{E, r1}
      cast_i{u32, f}
    }
  }
}

export{'avx2_squeeze_i8',   squeeze{arch_defvw, i8,  0, 0}}
export{'avx2_squeeze_i16',  squeeze{arch_defvw, i16, 0, 0}}
export{'avx2_squeeze_i32',  squeeze{arch_defvw, i32, 0, 0}}
export{'avx2_squeeze_f64',  squeeze{arch_defvw, f64, 0, 0}}
export{'avx2_squeeze_numB', squeeze{arch_defvw, f64, 0, 1}}

export{'avx2_squeeze_c16',  squeeze{arch_defvw, u16, 1, 0}}
export{'avx2_squeeze_c32',  squeeze{arch_defvw, u32, 1, 0}}
export{'avx2_squeeze_chrB', squeeze{arch_defvw, u64, 1, 1}}
