include './base'
include './sse3'
include './avx'
include './avx2'
include './mask'
include 'util/tup'

def preserve_negative_zero = 0

def inRangeLen{x:T, start, count & issigned{eltype{T}}} = {
  def TU = ty_u{T}
  (TU~~(x-broadcast{T,start})) < broadcast{TU,count}
}
def inRangeLen{x:T, start, count & isunsigned{eltype{T}}} = {
  def TS = ty_s{T}
  def h = 1 << (width{eltype{T}}-1)
  (TS~~x)-broadcast{TS,start-h} < broadcast{TS,count-h}
}
def inRangeIncl{x:T, start, end} = inRangeLen{x, start, end-start+1}
def inRangeExcl{x:T, start, end} = inRangeLen{x, start, end-start}

def isSNaN{x:T & isunsigned{eltype{T}}} = inRangeLen{x<<1, (0xFFE<<52)+2, (1<<52)-2}

def fold{F, x:T} = {
  show{'WARNING: using fallback fold'}
  def E = eltype{T}
  r:E = 0
  each{{i} => { r = F{r, cast_i{E, extract{x, i}}, iota{vcount{T}}} }}
  promote{u32, r}
}
def fold{F, x:T & w128{T}} = {
  c:= x
  def EW = width{eltype{T}}
  if (EW<=64) c = F{c, shuf{[4]u32, c, 4b1032}}
  if (EW<=32) c = F{c, shuf{[4]u32, c, 4b2301}}
  if (EW<=16) c = F{c, sel{[16]u8, c, make{[16]i8, iota{16}^2}}}
  if (EW<=8)  c = F{c, sel{[16]u8, c, make{[16]i8, iota{16}^1}}}
  extract{c, 0}
}
def fold{F, x:T & w256{T}} = fold{F, F{half{x, 0}, half{x, 1}}}

squeeze{vw, X0}(x0:*u8, len:Size) : u32 = {
  def X = tern{X0==u64, f64, X0}
  def bulk = vw / width{X}
  def XV = [bulk]X
  def E = tern{X==f64, u32, ty_u{X}}
  def EV = [bulk]E
  xp:= *X~~x0
  r1:= broadcast{EV, 0}
  if (X==i8) {
    maskedLoop{bulk, len, {i, M} => {
      v0:= loadBatch{xp, i, XV}
      if (anyne{broadcast{EV, 0xfe} & EV~~v0, broadcast{EV, 0}, M}) return{2}
    }}
    0
  } else {
    maskedLoop{bulk, len, {i, M} => {
      v0:= loadBatch{xp, i, XV}
      def toint{x:T & isint{eltype{T}}} = x
      def toint{flt:T & X==f64} = {
        int:= cvt2{i32, flt}
        
        def conv{x} = tern{preserve_negative_zero, ty_u{x}, x}
        
        if (anyne{conv{flt}, conv{cvt2{f64, int}}, M}) { # is any not an integer
          if (X0==f64) return{0xffff_ffff}
          
          maskedLoop{bulk, i, len, {i, M} => {
            def XU = [bulk]u64
            v:= XU ~~ loadBatch{xp, i, XV}
            if (any{M{isSNaN{v}}}) return{0xffff_ffff} # is any not even a float
          }}
          return{0xffff_fffe}
        }
        int
      }
      v1:= toint{v0}
      r1|= M{(broadcast{EV, ~E~~1} & EV~~v1) ^ EV~~(v1 >> (width{X}-1))}
    }}
    promote{u32, fold{|, r1}}
  }
}

'avx2_squeeze_i8'  = squeeze{256, i8 }
'avx2_squeeze_i16' = squeeze{256, i16}
'avx2_squeeze_i32' = squeeze{256, i32}
'avx2_squeeze_f64' = squeeze{256, f64}
'avx2_squeeze_B'   = squeeze{256, u64}
#'avx2_squeeze_u16' = squeeze{256, u16}
#'avx2_squeeze_u32' = squeeze{256, u32}