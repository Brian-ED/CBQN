include './base'
include './cbqnDefs'
include './f64'
if (hasarch{'X86_64'}) {
  include './sse3'
  include './avx'
  include './avx2'
} else if (hasarch{'AARCH64'}) {
  include './neon'
}
include './mask'
include './bitops'

# Group l (power of 2) elements into paired groups of length o
# e.g. pairs{2, iota{8}} = {{0,1,4,5}, {2,3,6,7}}
def pairs{o, x} = {
  def i = iota{tuplen{x}/2}
  def g = 2*i - i%o
  tupsel{tup{g, g+o}, x}
}
def unpack_pass{o, x} = merge{...each{unpackQ, ...pairs{o, x}}}
def permute_pass{o, x} = {
  def p = pairs{o, x}
  def h{s} = each{{a,b}=>emit{[8]i32, '_mm256_permute2f128_si256', a,b,s}, ...p}
  merge{h{16b20}, h{16b31}}
}

def ktest{a,l,T}{x} = {
  if (hasarch{a} and tuplen{x}==l and type{tupsel{0,x}}==T) 1 else 0
}

def vtranspose{x & ktest{'X86_64',8,[8]i32}{x}} = {
  permute_pass{4, unpack_pass{2, unpack_pass{1, x}}}
}
def vtranspose{x & ktest{'X86_64',4,[4]i64}{x}} = {
  permute_pass{2, unpack_pass{1, x}}
}

def vtranspose2{x & ktest{'X86_64',8,[16]i16}{x}} = {
  def r = unpack_pass{4, unpack_pass{2, unpack_pass{1, x}}}
  each{bind{~~,[16]i16}, r}
}
def load2{a:T, b:T & w128i{eltype{T}}} = {
  def V = eltype{T}
  emit{[2*vcount{V}](eltype{V}), '_mm256_loadu2_m128i', b, a}
}
def store2{a:T, b:T, v:T2 & w128i{eltype{T}} & w256{T2}} = {
  each{{p, i} => store{p, 0, half{v, i}}, tup{a,b}, iota{2}}
}



def for_mult{k}{vars,begin,end,block} = {
  assert{begin == 0}
  @for (i to end/k) exec{k*i, vars, block}
}

def mat_at{rp,xp,w,h}{x,y} = tup{xp + y*w + x, rp + x*h + y}

# Scalar transpose defined in C
def call_base{T} = {
  def ts = if (T==i8) 'i8' else if (T==i16) 'i16' else if (T==i32) 'i32' else 'i64'
  {...a} => emit{void, merge{'base_transpose_',ts}, ...a}
}
def small_transpose_out{T, k, rp, xp, w, h} = {
  if (w<k or h<k) { call_base{T}{rp, xp, w, h, w, h}; return{} }
}
def edge_transpose{T, k, rp, xp, w, h} = {
  def tr{...a} = call_base{T}{...a, w, h}
  wo := w%k; ws := w-wo; if (wo) tr{rp+h*ws, xp+  ws, wo, h }
  ho := h%k; hs := h-ho; if (ho) tr{rp+  hs, xp+w*hs, ws, ho}
}

fn transpose{T, k}(r0:*void, x0:*void, w:u64, h:u64) : void = {
  rp:*T = *T~~r0
  xp:*T = *T~~x0
  small_transpose_out{T, k, rp, xp, w, h}
  def at = mat_at{rp,xp,w,h}
  def VT = [k]T
  
  # Cache line info
  def line_bytes = 64
  def line_elts = line_bytes / (width{T}/8)
  def line_vecs = line_bytes / (width{VT}/8)
  
  if (h&(line_elts-1) != 0) {
    @for_mult{k} (y to h) {
      @for_mult{k} (x to w) {
        {xpo,rpo} := at{x, y}
        def xvs = each{{i}=>load{*VT~~(xpo+i*w), 0}, iota{k}}
        def rvs = vtranspose{xvs}
        each{{i,v}=>store{*VT~~(rpo+i*h), 0, v}, iota{k}, rvs}
      }
    }
  } else {
    # Result rows are aligned with each other so it's possible to
    # write a full cache line at a time
    # This case is here to mitigate cache associativity problems at
    # at multiples of 256 or so, but it's faster whenever it applies
    def store_line{p, vs} = each{bind{store,p}, iota{line_vecs}, vs}
    def get_lines{loadx} = {
      def vt{i} = vtranspose{each{loadx, k*i + iota{k}}}
      each{tup, ...each{vt, iota{line_vecs}}}
    }
    ro := tail{6, -u64~~r0} / (width{T}/8)  # Offset to align within cache line; assume elt-aligned
    wh := w*h
    yn := h
    if (ro != 0) {
      ra := line_elts - ro
      y := h - ra
      rpo := rp + y  # Cache aligned
      rpe := rpo + (w-1)*h
      # Part of first and last result row aren't covered by the split loop
      def trtail{dst, src, len} = @for (i to len) store{dst, i, load{src, w*i}}
      trtail{rp, xp, ro}
      trtail{rpe, xp + y*w + w-1, ra}
      # Transpose first few rows and last few rows together
      @for_mult{k} (x to w) {
        o := w*y + x
        def loadx{_} = {
          l:=load{*VT~~(xp+o)}
          o+=w; if (o>wh-k) o -= wh-1  # Jump from last source row to first, shifting right 1
          l
        }
        def rls = get_lines{loadx}  # 4 rows of 2 vectors each
        each{{i,v} => {if (i<3 or rpo<rpe) store_line{*VT~~rpo, v}; rpo+=h}, iota{k}, rls}
      }
      --yn  # One strip handled
    }
    @for_mult{line_elts} (y0 to yn) { y := y0 + ro
      @for_mult{k} (x to w) {
        {xpo,rpo} := at{x, y}
        def rls = get_lines{{i} => load{*VT~~(xpo+i*w), 0}}
        each{{i,v} => store_line{*VT~~(rpo+i*h), v}, iota{k}, rls}
      }
    }
  }
  
  edge_transpose{T, k, rp, xp, w, h}
}

fn transpose{T, k, m==2}(r0:*void, x0:*void, w:u64, h:u64) : void = {
  rp:*T = *T~~r0
  xp:*T = *T~~x0
  small_transpose_out{T, k, rp, xp, w, h}
  def at = mat_at{rp,xp,w,h}
  def d = m*k
  def VT = [d]T
  def HT = [k]T
  
  @for_mult{d} (y to h) {
    @for_mult{k} (x to w) {
      {xpo, rpo} := at{x, y}
      def xvs = each{{i}=>{p:=xpo+i*w; load2{*HT~~p, *HT~~(p+k*w)}}, iota{k}}
      def rvs = vtranspose2{xvs}
      each{{i,v}=>store{*VT~~(rpo+i*h), 0, v}, iota{k}, rvs}
    }
  }
  if ((h & k) != 0) { y := h-h%d
    @for_mult{k} (x to w) {
      {xpo, rpo} := at{x, y}
      def lw = k*width{T}
      def xvs = each{{i}=>loadLow{*VT~~(xpo+i*w), lw}, iota{k}}
      def rvs = vtranspose2{xvs}
      each{{i,v}=>storeLow{*VT~~(rpo+i*h), lw, v}, iota{k}, rvs}
    }
  }
  
  edge_transpose{T, k, rp, xp, w, h}
}

def vtranspose{x & ktest{'X86_64',8,[32]i8}{x}} = {
  def r = unpack_pass{4, unpack_pass{2, unpack_pass{1, x}}}
  each{{v}=>[32]i8~~shuf{[4]i64, v, 4b3120}, r}
}
fn transpose{T==i8, k}(r0:*void, x0:*void, w:u64, h:u64) : void = {
  rp:*T = *T~~r0
  xp:*T = *T~~x0
  small_transpose_out{T, k, rp, xp, w, h}
  def at = mat_at{rp,xp,w,h}
  def VT = [k]T
  
  @for_mult{k} (y to h) {
    @for_mult{k} (x to w) {
      {xpo, rpo} := at{x, y}
      def s = k/2
      def xvs = each{{i}=>{p:=xpo+i*w; load2{*VT~~p, *VT~~(p+s*w)}}, iota{s}}
      def rvs = vtranspose{xvs}
      each{{i,v}=>{p:=rpo+2*i*h; store2{*VT~~p, *VT~~(p+h), v}}, iota{s}, rvs}
    }
  }
  
  edge_transpose{T, k, rp, xp, w, h}
}

export{'simd_transpose_i8',  transpose{i8 , 16}}
export{'simd_transpose_i16', transpose{i16, 8, 2}}
export{'simd_transpose_i32', transpose{i32, 8}}
export{'simd_transpose_i64', transpose{i64, 4}}
